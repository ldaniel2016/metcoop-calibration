# -*- coding: utf-8 -*-
"""meps_training_ens_mean_sd_nn_timesplit_train_val_test.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1HPdC1wIyIJ6_XrSiMaVUg-Ob_GF6NkH8

#Post processing of T2m using linear regression and  neural network. The train and test data are split on month days #

In this notebook, we postprocess T2m using the observed 2m temperature  TA and the ensemble means and sds of T2m, forecast temperature at 2m and other forecast variables from MEPS. We use linear regression and neural network based training. The loss function is CRPS. The codes are adapted from the paper Neural Networks for Postprocessing Ensemble Weather Forecasts by Rasp etal, 2018
We split the training, validation and test sets based on specific days of the month
"""

# Commented out IPython magic to ensure Python compatibility.
# %load_ext autoreload
# %autoreload 2

# Commented out IPython magic to ensure Python compatibility.
# %matplotlib inline
from matplotlib import pyplot
import pandas as pd
import numpy as np
import json
import io

# Imports
import sys

import keras
from keras.layers import Input, Dense, merge, Embedding, Flatten, Dropout, \
    SimpleRNN, LSTM, TimeDistributed, GRU, Dropout, Masking
from keras.layers.merge import Concatenate
from keras.models import Model, Sequential
import keras.backend as K
from keras.callbacks import EarlyStopping
from keras.optimizers import SGD, Adam

"""## Data preparation. Download the file trdata_spatial_lcc_T2m_00+036.csv from googledrive. Find the ensemble means and sds of variables"""

# Code to read csv file into Colaboratory:
#https://towardsdatascience.com/3-ways-to-load-csv-files-into-colab-7c14fcbdcb92
!pip install -U -q PyDrive
from pydrive.auth import GoogleAuth
from pydrive.drive import GoogleDrive
from google.colab import auth
from oauth2client.client import GoogleCredentials# Authenticate and create the PyDrive client.
auth.authenticate_user()
gauth = GoogleAuth()
gauth.credentials = GoogleCredentials.get_application_default()
drive = GoogleDrive(gauth)

link = "https://drive.google.com/open?id=1-1MJtL-ZnmTWNLt6wSuxufNoQDnrOzdC"

fluff, id = link.split('=')
print (id) # Verify that you have everything after '='

downloaded = drive.CreateFile({'id':id}) 
downloaded.GetContentFile('trdata_spatial_lcc_T2m_00+036.csv')  
df = pd.read_csv('trdata_spatial_lcc_T2m_00+036.csv')

dfsaved = df
df.head(2)

# Removing the NAs
df = df.dropna(axis = 0)

print(df.shape, dfsaved.shape)

"""## Finding the ensemble means and sd"""

# This is to avoid the SettingWithCopyWarning. I did use the .loc to avoid this warning but it does not help. I checked the copying operation and it is correct
pd.set_option('mode.chained_assignment', None)

col = df.loc[: , "T0.0":"T0.9"]
df['T0mensmean'] = col.mean(axis=1)
df['T0menssd']   = col.std(axis= 1)

col = df.loc[: , "T2.0":"T2.9"]
df['T2mensmean'] = col.mean(axis=1)
df['T2menssd']   = col.std(axis= 1)

col = df.loc[: , "H2.0":"H2.9"]
df['H2mensmean'] = col.mean(axis=1)
df['H2menssd']   = col.std(axis= 1)

col = df.loc[: , "T.L850.0":"T.L850.9"]
df['T.L850ensmean'] = col.mean(axis=1)
df['T.L850enssd']   = col.std(axis= 1)

col = df.loc[: , "T.L925.0":"T.L925.9"]
df['T.L925ensmean'] = col.mean(axis=1)
df['T.L925enssd']   = col.std(axis= 1)

col = df.loc[: , "WS10.0":"WS10.9"]
df['WS10ensmean'] = col.mean(axis=1)
df['WS10enssd']   = col.std(axis= 1)

col = df.loc[: , "WD10.0":"WD10.9"]
df['WD10ensmean'] = col.mean(axis=1)
df['WD10enssd']   = col.std(axis= 1)

col = df.loc[: , "TCC.0":"TCC.9"]
df['TCCensmean'] = col.mean(axis=1)
df['TCCenssd']   = col.std(axis= 1)

col = df.loc[: , "MCC.0":"MCC.9"]
df['MCCensmean'] = col.mean(axis=1)
df['MCCenssd']   = col.std(axis= 1)

col = df.loc[: , "LCC.0":"LCC.9"]
df['LCCensmean'] = col.mean(axis=1)
df['LCCenssd']   = col.std(axis= 1)

col = df.loc[: , "MSLP.0":"MSLP.9"]
df['MSLPensmean'] = col.mean(axis=1)
df['MSLPenssd']   = col.std(axis= 1)

df['dELEV'] = (df['SG.0']/9.81) - df['elev']

df_ens = df[['TIME','x','y','TA', 'dELEV','T0mensmean', 'T0menssd','T2mensmean', 'T2menssd', 'H2mensmean', 'H2menssd', 'T.L850ensmean', 'T.L850enssd',  'T.L925ensmean', 'T.L925enssd', 'WS10ensmean', 'WS10enssd', 'WD10ensmean', 'WD10enssd','TCCensmean', 'TCCenssd','MCCensmean', 'MCCenssd','LCCensmean', 'LCCenssd', 'MSLPensmean', 'MSLPenssd', 'RR_6'	 ]]

df_ens.head(2)

df_ens['TIME'] = pd.to_datetime(df_ens['TIME'],unit='s')
df_ens.head(2)

df_ens.shape

df_ens.columns

"""## CRPS loss function from RASP paper"""

# Imports
import sys

import keras
from keras.layers import Input, Dense, merge, Embedding, Flatten, Dropout, \
    SimpleRNN, LSTM, TimeDistributed, GRU, Dropout, Masking
from keras.layers.merge import Concatenate
from keras.models import Model, Sequential
import keras.backend as K
from keras.callbacks import EarlyStopping
from keras.optimizers import SGD, Adam

"""
Definition of CRPS loss function.
"""
import keras
import keras.backend as K
import numpy as np
# Import erf depending on whether we use the theano or tensorflow backend
if keras.backend.backend() == 'tensorflow':
  from tensorflow import erf
else:
  from theano.tensor import erf


def crps_cost_function(y_true, y_pred, theano=False):
    """Compute the CRPS cost function for a normal distribution defined by
    the mean and standard deviation.
    Code inspired by Kai Polsterer (HITS).
    Args:
        y_true: True values
        y_pred: Tensor containing predictions: [mean, std]
        theano: Set to true if using this with pure theano.
    Returns:
        mean_crps: Scalar with mean CRPS over batch
  """
    # Split input
    mu = y_pred[:, 0]
    sigma = y_pred[:, 1]
    # Ugly workaround for different tensor allocation in keras and theano
    if not theano:
        y_true = y_true[:, 0]   # Need to also get rid of axis 1 to match!

    # To stop sigma from becoming negative we first have to 
    # convert it the the variance and then take the square
    # root again. 
    var = K.square(sigma)
    # The following three variables are just for convenience
    loc = (y_true - mu) / K.sqrt(var)
    phi = 1.0 / np.sqrt(2.0 * np.pi) * K.exp(-K.square(loc) / 2.0)
    Phi = 0.5 * (1.0 + erf(loc / np.sqrt(2.0)))
    # First we will compute the crps for each input/target pairtrdata_00+036
    crps =  K.sqrt(var) * (loc * (2. * Phi - 1.) + 2 * phi - 1. / np.sqrt(np.pi))
    # Then we take the mean. The cost is now a scalartrdata_00+036
    return K.mean(crps)


def crps_cost_function_seq(y_true, y_pred):
    """Version of CRPS const function for sequence predictions.
    Here the input tensors have dimensions [sample, time_step].
    The output has the same dimensions so that keras can apply weights
    afterwards for missing data.vo
    Args:  
        y_true: True values with dimensions [sample, time_step, 1]
        y_pred: Predictions with dimensions [sample, time_step, [mean, std]]
    Returns:
        crps: CRPS with dimensions [sample, time_step]
    """
    # Split input
    mu = y_pred[:, :, 0]
    sigma = y_pred[:, :, 1]
    
    tar = y_true[:, :, 0]
    # [sample, time_step]

    # To stop sigma from becoming negative we first have to 
    # convert it the the variance and then take the square9.4 	
    # root again. 
    var = K.square(sigma)
    # The following three variables are just for convenience
    loc = (tar - mu) / K.sqrt(var)
    phi = 1.0 / np.sqrt(2.0 * np.pi) * K.exp(-K.square(loc) / 2.0)
    Phi = 0.5 * (1.0 + erf(loc / np.sqrt(2.0)))
    # First we will compute the crps for each input/target pair
    crps = K.sqrt(var) * (loc * (2. * Phi - 1.) + 2 * phi - 1. / np.sqrt(np.pi))

    # Here we do not take the mean because we want keras to be able to apply
    # weights afterwards!
    return crps


def approx_crps_cat(bin_width): 	
    """Wrapper to pass bin_width as an argument to the loss function.
    Args:
        bin_width: width of categorical bins
    Returns:
        loss_function: approximate crps_loss function with bin_width specified
    """ 
    
    def loss(y_true, y_pred):
        """Approximate CRPS function for categorical output.
        Args:
            y_true: One-hot-encoded output
            y_pred: Probability for each bin
        Returns:
            approx_crps: Approximate mean CRPS value for batch
        """
        # [sample, cat]
        cum_obs = K.cumsum(y_true, axis=1)
        cum_preds = K.cumsum(y_pred, axis=1)
        approx_crps = K.sum(K.square(cum_obs - cum_preds), axis=1) * bin_width
        return K.mean(approx_crps)
    return loss

def crps_normal(mu, sigma, y):
    """
    Compute CRPS for a Gaussian distribution. 
    """
    loc = (y - mu) / sigma
    crps = sigma * (loc * (2 * norm.cdf(loc) - 1) + 
                    2 * norm.pdf(loc) - 1. / np.sqrt(np.pi))
    return crps


def maybe_correct_cat_crps(preds, targets, bin_edges):
    """CRPS for categorical predictions. I think this is correct now.
    """
    # pdb.set_trace()
    # Convert input arrays
    preds = np.array(np.atleast_2d(preds), dtype='float')
    targets = np.array(np.atleast_1d(targets), dtype='float')

    # preds [sample, bins]
    # Find insert index
    mat_bins = np.repeat(np.atleast_2d(bin_edges), targets.shape[0], axis=0)
    b = mat_bins.T - targets
    b[b < 0] = 999
    insert_idxs = np.argmin(b, axis=0)

    # Insert
    ins_bin_edges = np.array([np.insert(np.array(bin_edges, dtype=float),
                                        insert_idxs[i], targets[i])
                              for i in range(targets.shape[0])])
    ins_preds = np.array(
        [np.insert(preds[i], insert_idxs[i], preds[i, insert_idxs[i] - 1])
         for i in range(targets.shape[0])])

    # Get obs
    bin_obs = np.array([(ins_bin_edges[i, :-1] <= targets[i]) &
                        (ins_bin_edges[i, 1:] > targets[i])
                        for i in range(targets.shape[0])], dtype=int)

    # Cumsum with weights
    ins_preds *= np.diff(ins_bin_edges, axis=1)
    cum_bin_obs = np.cumsum(bin_obs, axis=1)
    cum_probs = np.cumsum(ins_preds, axis=1)
    cum_probs = (cum_probs.T / cum_probs[:, -1]).T

    # Get adjusted preds
    adj_cum_probs = np.concatenate((np.zeros((cum_probs.shape[0], 1)),
                                    cum_probs), axis=1)
    # Compute squared area for each bin
    sq_list = []
    for i in range(cum_bin_obs.shape[1]):
        x_l = np.abs(cum_bin_obs[:, i] - adj_cum_probs[:, i])
        x_r = np.abs(cum_bin_obs[:, i] - adj_cum_probs[:, i + 1])
        sq = 1./3. * (x_l ** 2 + x_l * x_r + x_r ** 2)
        sq_list.append(sq)

    # Compute CRPS
    crps = np.sum(np.array(sq_list).T * np.diff(ins_bin_edges, axis=1), axis=1)
    return np.mean(crps)

"""## CRPS for Raw ensemble values of T2mensmean and . CRPS measures the difference between observed CDF and the forecast CDF. Perfect score is 0"""

mu = np.mean(df_ens['T2mensmean'])
sigma = np.std(df_ens['T2menssd'])
(np.mean(df_ens['T2mensmean']), np.std(df_ens['T2menssd']))

from scipy.stats import norm
crps_raw = np.mean(crps_normal(df_ens['T2mensmean'], df_ens['T2menssd'], df['TA']))
crps_raw

"""## Postprocessing of T2m ensemble mean and standardard deviation with  MEPS T2m and other variables"""

#Splitting, Scaling and standardindization Random split This cell not used now

from sklearn.model_selection import train_test_split

train, test = train_test_split(df_ens, test_size=0.3)
print(train.shape, test.shape)
train_X = train.drop('TA',1)
train_y = train[['TA']]
train_X.head(2)
train_y.head(2)

test_X = test.drop('TA',1)
test_y = test[['TA']]
test_X.head(2)
test_y.head(2)

from sklearn.preprocessing import StandardScaler
# create scaler
scaler = StandardScaler()
# fit scaler on train data
scaler.fit(train_X)
# apply transform
train_standardized_X  = scaler.transform(train_X)
# fit scaler on test data
scaler.fit(test_X)
# apply transform
test_standardized_X  = scaler.transform(test_X)

df_ens.shape

"""## Splitting to train, validation and test sets using days of the week
Here train, validation and test has (89838, 28), (12653, 28), (25260, 28))
"""

test = df_ens[df_ens['TIME'].dt.day.isin({5,10,15,20,25,30})]
train_val = df_ens[~df_ens.isin(test)].dropna()
(train_val.shape, test.shape)

validation = train_val[train_val['TIME'].dt.day.isin({4,12,20,28})]
train = train_val[~train_val.isin(validation)].dropna()
(train.shape, validation.shape, test.shape)

train_X = train.drop(['TIME','TA'],1)
train_X.shape
train_y = train[['TA']]

train_X.head(2)
train_y.head(2)

test_X = test.drop(['TIME','TA'],1)
test_y = test[['TA']]

valid_X = validation.drop(['TIME','TA'],1)
valid_y = validation[['TA']]

(train_X.shape, train_y.shape, valid_X.shape, valid_y.shape, test_X.shape,test_y.shape)

"""## Standardizing  the train_X, valid_X and test_X"""

train_std_X= (train_X - train_X.mean())/train_X.std()
test_std_X  = (test_X - test_X.mean())/test_X.std()
valid_std_X= (valid_X - valid_X.mean())/valid_X.std()

train_std_X .head(2)

train_y.head(2)

"""## **Post processing using the Fully connected or the Linear Regression  model and the neural network model**

## Fully-Connected FC/LR model
"""

#from ppnn paper
def build_fc_model(n_features, n_outputs, compile=False, optimizer='adam',
                   lr=0.1, loss=crps_cost_function):
    """Build (and compile) fully connected linear network.

    Args:
        n_features: Number of features
        n_outputs: Number of outputs
        compile: If true, compile model
        optimizer:  Name of optimizer
        lr: learning rate
        loss: loss function

    Returns:
        model: Keras model
    """

    inp = Input(shape=(n_features,))
    x = Dense(n_outputs, activation='linear')(inp)
    model = Model(inputs=inp, outputs=x)

    if compile:
        #opt = keras.optimizers.__dict__[optimizer](lr=lr)
        model.compile(optimizer='adam', loss=loss)
    return model

input_features =  len(train_std_X.columns)
print(input_features)
fc_model =build_fc_model(input_features, 2, compile=True)
fc_model.summary()

history = fc_model.fit(train_std_X, train_y, epochs=500, batch_size=50, validation_data=(valid_std_X,valid_y), verbose=2, shuffle=False)

#CRPS of train and test data days split
(fc_model.evaluate(train_std_X, train_y, batch_size = 50, verbose=0), fc_model.evaluate(test_std_X, test_y, batch_size = 50, verbose=0))

#CRPS of train and test data random split
(fc_model.evaluate(train_std_df_X, train_y, batch_size = 50, verbose=0), fc_model.evaluate(test_std_df_X, test_y, batch_size = 50, verbose=0))

"""## Feature importance"""

# Commented out IPython magic to ensure Python compatibility.
#Feature importance


#Feature importance for standardized scaled
# %matplotlib inline
import matplotlib.pyplot as plt
import seaborn as sns

# slice error add iloc
def eval_shuf(m, idx, emb=False):
    x_shuf = test_std_X.copy()
    x_shuf.iloc[:, idx] = np.random.permutation(x_shuf.iloc[:, idx])
    x = x_shuf
    return m.evaluate(x, test_y, 4096, 0)
def perm_imp(m):
    scores = [eval_shuf(m, i) for i in range(len(test_X.columns))]
    fimp = np.array(scores) - ref_score
    df = pd.DataFrame(columns=['Feature', 'Importance'])
    df['Feature'] = test_std_X.columns; df['Importance'] = fimp
    return df
def perm_imp(m):
    scores = [eval_shuf(m, i) for i in range(len(test_X.columns))]
    fimp = np.array(scores) - ref_score
    df = pd.DataFrame(columns=['Feature', 'Importance'])
    df['Feature'] = test_std_X.columns; df['Importance'] = fimp
    return df

#fimp for fc
ref_score = fc_model.evaluate(test_std_X, test_y, batch_size = 50, verbose=0)

fimp_fc_standardized_model = perm_imp(fc_model)

#fimp for fc
ref_score = fc_model.evaluate(test_std_X, test_y, batch_size = 50, verbose=0)

fimp_fc_standardized_model = perm_imp(fc_model)
fig, ax = plt.subplots(figsize=(12, 5))
sns.barplot(data=fimp_fc_standardized_model, y='Importance', x='Feature', ax=ax)
plt.xticks(rotation=90);

"""## Neural network (NN) model"""

#Neural network, one hidden layer

def build_hidden_model(n_features, n_outputs, hidden_nodes, compile=False,
                       optimizer='adam', lr=0.01, loss=crps_cost_function,
                       activation='relu'):
    """Build (and compile) a neural net with hidden layers
    Args:
        n_features: Number of features
        n_outputs: Number of outputs
        hidden_nodes: int or list of hidden nodes
        compile: If true, compile model
        optimizer: Name of optimizer
        lr: learning rate
        loss: loss function
    Returns:
        model: Keras model
    """
    if type(hidden_nodes) is not list:
        hidden_nodes = [hidden_nodes]
    inp = Input(shape=(n_features,))
    x = Dense(hidden_nodes[0], activation=activation)(inp)
    if len(hidden_nodes) > 1:
        for h in hidden_nodes[1:]:
            x = Dense(h, activation=activation)(x)
    x = Dense(n_outputs, activation='linear')(x)
    model = Model(inputs=inp, outputs=x)

    if compile:
        opt = keras.optimizers.__dict__[optimizer](lr=lr)
        model.compile(optimizer=opt, loss=loss)
    return model

input_features =  len(train_std_X.columns)
print(input_features)
hidden_model = build_hidden_model(input_features, 2, hidden_nodes=[50], compile=True)
hidden_model.summary()

hidden_model.compile( keras.optimizers.Adam(0.001), loss=crps_cost_function)

# Commented out IPython magic to ensure Python compatibility.
# #Hidden model with train and test data days split
# %%time
# hidden_model.fit(train_std_X, train_y, epochs=500, batch_size = 50,
#                  validation_data=(valid_std_X, valid_y), verbose=2, shuffle=False)

#CRPS of train and test data 500 epochs ensemble mean and sd
(hidden_model.evaluate(train_std_X, train_y, batch_size = 50, verbose=0), hidden_model.evaluate(test_std_X, test_y, batch_size = 50, verbose=0))

#CRPS of train and test data 500 epochs ensemble mean and sd with random split
(hidden_model.evaluate(train_std_X, train_y, batch_size = 50, verbose=0), hidden_model.evaluate(test_df_X, test_y, batch_size = 50, verbose=0))

"""## Feature importance for NN model"""

#fimp for nn with distance to coast 500 eposchs train test days split

ref_score = hidden_model.evaluate(test_std_X, test_y, batch_size = 50, verbose=0)
fimp_nn_standardized_model = perm_imp(hidden_model)
fig, ax = plt.subplots(figsize=(12, 5))
sns.barplot(data=fimp_nn_standardized_model, y='Importance', x='Feature', ax=ax)
plt.xticks(rotation=90);

# Feature importance train test days split
comb_nn_df = pd.DataFrame(data=fimp_nn_standardized_model['Feature']); 
comb_nn_df['nn_std_importance'] = fimp_nn_standardized_model['Importance']
comb_nn_df.sort_values('nn_std_importance', ascending=False, inplace=True)
comb_nn_df['FC/LR_std_importance'] = fimp_fc_standardized_model['Importance']
comb_nn_df.head(30)

#fimp for nn with distance to coast 500 epochs train test random split

ref_score = hidden_model.evaluate(test_std_df_X, test_y, batch_size = 50, verbose=0)
fimp_nn_standardized_model = perm_imp(hidden_model)
fig, ax = plt.subplots(figsize=(12, 5))
sns.barplot(data=fimp_nn_standardized_model, y='Importance', x='Feature', ax=ax)
plt.xticks(rotation=90);

"""## Feature importance tabular form for FC/LR and NN models"""

# Feature importance train test random split
comb_nn_df = pd.DataFrame(data=fimp_nn_standardized_model['Feature']); 
comb_nn_df['nn_std_importance'] = fimp_nn_standardized_model['Importance']
comb_nn_df.sort_values('nn_std_importance', ascending=False, inplace=True)
comb_nn_df['FC/LR_std_importance'] = fimp_fc_standardized_model['Importance']
comb_nn_df.head(30)

hidden_model.compile( keras.optimizers.Adam(0.0001), loss=crps_cost_function)

# Commented out IPython magic to ensure Python compatibility.
# #Hidden model with train and test data days split
# %%time
# hidden_model.fit(train_std_X, train_y, epochs=500, batch_size = 50,
#                  validation_data=(valid_std_X, valid_y), verbose=2, shuffle=False)

#CRPS of train and test data 500 epochs ensemble mean and sd with optimizer 0.0001
(hidden_model.evaluate(train_std_X, train_y, batch_size = 50, verbose=0), hidden_model.evaluate(test_std_X, test_y, batch_size = 50, verbose=0))

#CRPS of train and test data 500 epochs ensemble mean and sd with optimizer 0.0005
(hidden_model.evaluate(train_std_X, train_y, batch_size = 50, verbose=0), hidden_model.evaluate(test_std_X, test_y, batch_size = 50, verbose=0))

