{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "meps_training_ens_mean_sd_nn.ipynb",
      "provenance": [],
      "collapsed_sections": [
        "Bw96ehK32lO8",
        "0rzdExPD2w4k",
        "Ug5aG6IOKdF-"
      ],
      "machine_shape": "hm"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eVuUSiGXT_-g",
        "colab_type": "text"
      },
      "source": [
        "#Post processing of T2m using linear regression and  neural network#\n",
        "\n",
        "In this notebook, we postprocess T2m using the observed 2m temperature  TA and the ensemble means and sds of T2m, forecast temperature at 2m and other forecast variables from MEPS. We use linear regression and neural network based training. The loss function is CRPS. The codes are adapted from the paper Neural Networks for Postprocessing Ensemble Weather Forecasts by Rasp etal, 2018\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qh67IKgFNREL",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "%load_ext autoreload\n",
        "%autoreload 2\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ot9tJ92CNwIP",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "%matplotlib inline\n",
        "from matplotlib import pyplot\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import json\n",
        "import io"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7k-Bo0jUNzpB",
        "colab_type": "code",
        "outputId": "062643a4-7b15-412e-b6fc-f05672161a7c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 81
        }
      },
      "source": [
        "# Imports\n",
        "import sys\n",
        "\n",
        "import keras\n",
        "from keras.layers import Input, Dense, merge, Embedding, Flatten, Dropout, \\\n",
        "    SimpleRNN, LSTM, TimeDistributed, GRU, Dropout, Masking\n",
        "from keras.layers.merge import Concatenate\n",
        "from keras.models import Model, Sequential\n",
        "import keras.backend as K\n",
        "from keras.callbacks import EarlyStopping\n",
        "from keras.optimizers import SGD, Adam"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Using TensorFlow backend.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<p style=\"color: red;\">\n",
              "The default version of TensorFlow in Colab will soon switch to TensorFlow 2.x.<br>\n",
              "We recommend you <a href=\"https://www.tensorflow.org/guide/migrate\" target=\"_blank\">upgrade</a> now \n",
              "or ensure your notebook will continue to use TensorFlow 1.x via the <code>%tensorflow_version 1.x</code> magic:\n",
              "<a href=\"https://colab.research.google.com/notebooks/tensorflow_version.ipynb\" target=\"_blank\">more info</a>.</p>\n"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pfUO267i1St-",
        "colab_type": "text"
      },
      "source": [
        "## Data preparation. Download the file trdata_spatial_lcc_T2m_00+036.csv from googledrive. Find the ensemble means and sds of variables"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_KBWUmv21SX9",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Code to read csv file into Colaboratory:\n",
        "#https://towardsdatascience.com/3-ways-to-load-csv-files-into-colab-7c14fcbdcb92\n",
        "!pip install -U -q PyDrive\n",
        "from pydrive.auth import GoogleAuth\n",
        "from pydrive.drive import GoogleDrive\n",
        "from google.colab import auth\n",
        "from oauth2client.client import GoogleCredentials# Authenticate and create the PyDrive client.\n",
        "auth.authenticate_user()\n",
        "gauth = GoogleAuth()\n",
        "gauth.credentials = GoogleCredentials.get_application_default()\n",
        "drive = GoogleDrive(gauth)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "i5TI54en92Om",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "link = \"https://drive.google.com/open?id=1-1MJtL-ZnmTWNLt6wSuxufNoQDnrOzdC\""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AF_BvLs1917r",
        "colab_type": "code",
        "outputId": "de6d8798-8300-4ba9-ec22-bc3d94ed04a7",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "fluff, id = link.split('=')\n",
        "print (id) # Verify that you have everything after '='"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "1-1MJtL-ZnmTWNLt6wSuxufNoQDnrOzdC\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sh8FdM9s91uO",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "downloaded = drive.CreateFile({'id':id}) \n",
        "downloaded.GetContentFile('trdata_spatial_lcc_T2m_00+036.csv')  \n",
        "df = pd.read_csv('trdata_spatial_lcc_T2m_00+036.csv')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_tma6QnPA8Xy",
        "colab_type": "code",
        "outputId": "6f5d64d0-6297-4e25-dcc2-da720d0cfb9c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 196
        }
      },
      "source": [
        "dfsaved = df\n",
        "df.head(2)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>LOC</th>\n",
              "      <th>TIME</th>\n",
              "      <th>TIME.REF</th>\n",
              "      <th>LT</th>\n",
              "      <th>SG.0</th>\n",
              "      <th>T2.0</th>\n",
              "      <th>T2.1</th>\n",
              "      <th>T2.2</th>\n",
              "      <th>T2.3</th>\n",
              "      <th>T2.4</th>\n",
              "      <th>T2.5</th>\n",
              "      <th>T2.6</th>\n",
              "      <th>T2.7</th>\n",
              "      <th>T2.8</th>\n",
              "      <th>T2.9</th>\n",
              "      <th>T0.0</th>\n",
              "      <th>T0.1</th>\n",
              "      <th>T0.2</th>\n",
              "      <th>T0.3</th>\n",
              "      <th>T0.4</th>\n",
              "      <th>T0.5</th>\n",
              "      <th>T0.6</th>\n",
              "      <th>T0.7</th>\n",
              "      <th>T0.8</th>\n",
              "      <th>T0.9</th>\n",
              "      <th>H2.0</th>\n",
              "      <th>H2.1</th>\n",
              "      <th>H2.2</th>\n",
              "      <th>H2.3</th>\n",
              "      <th>H2.4</th>\n",
              "      <th>H2.5</th>\n",
              "      <th>H2.6</th>\n",
              "      <th>H2.7</th>\n",
              "      <th>H2.8</th>\n",
              "      <th>H2.9</th>\n",
              "      <th>T.L850.0</th>\n",
              "      <th>T.L850.1</th>\n",
              "      <th>T.L850.2</th>\n",
              "      <th>T.L850.3</th>\n",
              "      <th>T.L850.4</th>\n",
              "      <th>...</th>\n",
              "      <th>MSLP.2</th>\n",
              "      <th>MSLP.3</th>\n",
              "      <th>MSLP.4</th>\n",
              "      <th>MSLP.5</th>\n",
              "      <th>MSLP.6</th>\n",
              "      <th>MSLP.7</th>\n",
              "      <th>MSLP.8</th>\n",
              "      <th>MSLP.9</th>\n",
              "      <th>LAT</th>\n",
              "      <th>LON</th>\n",
              "      <th>AMSL</th>\n",
              "      <th>TA</th>\n",
              "      <th>RR_6</th>\n",
              "      <th>x</th>\n",
              "      <th>y</th>\n",
              "      <th>name</th>\n",
              "      <th>elev</th>\n",
              "      <th>elev100m_meps</th>\n",
              "      <th>elev500m_meps</th>\n",
              "      <th>elev1km_meps</th>\n",
              "      <th>elev2.5km_meps</th>\n",
              "      <th>elev5km_meps</th>\n",
              "      <th>rough500m_meps</th>\n",
              "      <th>rough1km_meps</th>\n",
              "      <th>rough2.5km_meps</th>\n",
              "      <th>rough5km_meps</th>\n",
              "      <th>NSABS500m_meps</th>\n",
              "      <th>NSABS1km_meps</th>\n",
              "      <th>NSABS2.5km_meps</th>\n",
              "      <th>NSABS5km_meps</th>\n",
              "      <th>WEABS500m_meps</th>\n",
              "      <th>WEABS1km_meps</th>\n",
              "      <th>WEABS2.5km_meps</th>\n",
              "      <th>WEABS5km_meps</th>\n",
              "      <th>ZABS500m_meps</th>\n",
              "      <th>ZABS1km_meps</th>\n",
              "      <th>ZABS2.5km_meps</th>\n",
              "      <th>ZABS5km_meps</th>\n",
              "      <th>dist_to_coast</th>\n",
              "      <th>terrain_type_meps</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>51.800N 30.250E</td>\n",
              "      <td>1.504872e+09</td>\n",
              "      <td>1.504742e+09</td>\n",
              "      <td>36.0</td>\n",
              "      <td>1114.544312</td>\n",
              "      <td>17.044397</td>\n",
              "      <td>16.628595</td>\n",
              "      <td>16.837946</td>\n",
              "      <td>16.974847</td>\n",
              "      <td>17.102686</td>\n",
              "      <td>17.684412</td>\n",
              "      <td>16.138879</td>\n",
              "      <td>17.330255</td>\n",
              "      <td>16.959161</td>\n",
              "      <td>17.523859</td>\n",
              "      <td>19.408685</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>0.520012</td>\n",
              "      <td>0.627704</td>\n",
              "      <td>0.451124</td>\n",
              "      <td>0.519454</td>\n",
              "      <td>0.516697</td>\n",
              "      <td>0.539641</td>\n",
              "      <td>0.514709</td>\n",
              "      <td>0.520381</td>\n",
              "      <td>0.513829</td>\n",
              "      <td>0.557433</td>\n",
              "      <td>3.551</td>\n",
              "      <td>3.446</td>\n",
              "      <td>3.690</td>\n",
              "      <td>3.610</td>\n",
              "      <td>3.377</td>\n",
              "      <td>...</td>\n",
              "      <td>101343.835938</td>\n",
              "      <td>101121.507812</td>\n",
              "      <td>101432.500000</td>\n",
              "      <td>101262.304688</td>\n",
              "      <td>101293.78125</td>\n",
              "      <td>101340.523438</td>\n",
              "      <td>101196.070312</td>\n",
              "      <td>101210.03125</td>\n",
              "      <td>51.8</td>\n",
              "      <td>30.25</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>1.057841e+06</td>\n",
              "      <td>-1.160339e+06</td>\n",
              "      <td>BRAGIN</td>\n",
              "      <td>116</td>\n",
              "      <td>112.0</td>\n",
              "      <td>113.0</td>\n",
              "      <td>112.0</td>\n",
              "      <td>112.0</td>\n",
              "      <td>114.0</td>\n",
              "      <td>2.0</td>\n",
              "      <td>2.0</td>\n",
              "      <td>8.0</td>\n",
              "      <td>10.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>-0.5</td>\n",
              "      <td>-2.0</td>\n",
              "      <td>-4.5</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.5</td>\n",
              "      <td>-2.0</td>\n",
              "      <td>-2.5</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.707107</td>\n",
              "      <td>2.0</td>\n",
              "      <td>4.609772</td>\n",
              "      <td>556684</td>\n",
              "      <td>50.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>51.800N 30.250E</td>\n",
              "      <td>1.504958e+09</td>\n",
              "      <td>1.504829e+09</td>\n",
              "      <td>36.0</td>\n",
              "      <td>1114.544312</td>\n",
              "      <td>22.414880</td>\n",
              "      <td>22.958459</td>\n",
              "      <td>21.841669</td>\n",
              "      <td>22.291254</td>\n",
              "      <td>22.213190</td>\n",
              "      <td>21.955652</td>\n",
              "      <td>21.542596</td>\n",
              "      <td>22.999017</td>\n",
              "      <td>21.349359</td>\n",
              "      <td>21.407953</td>\n",
              "      <td>26.322351</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>0.448643</td>\n",
              "      <td>0.378334</td>\n",
              "      <td>0.525253</td>\n",
              "      <td>0.477166</td>\n",
              "      <td>0.439878</td>\n",
              "      <td>0.495448</td>\n",
              "      <td>0.490581</td>\n",
              "      <td>0.416223</td>\n",
              "      <td>0.522221</td>\n",
              "      <td>0.530758</td>\n",
              "      <td>8.412</td>\n",
              "      <td>8.635</td>\n",
              "      <td>8.197</td>\n",
              "      <td>9.102</td>\n",
              "      <td>7.823</td>\n",
              "      <td>...</td>\n",
              "      <td>101537.468750</td>\n",
              "      <td>101455.585938</td>\n",
              "      <td>101608.726562</td>\n",
              "      <td>101460.992188</td>\n",
              "      <td>101571.31250</td>\n",
              "      <td>101580.984375</td>\n",
              "      <td>101460.054688</td>\n",
              "      <td>101557.31250</td>\n",
              "      <td>51.8</td>\n",
              "      <td>30.25</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>1.057841e+06</td>\n",
              "      <td>-1.160339e+06</td>\n",
              "      <td>BRAGIN</td>\n",
              "      <td>116</td>\n",
              "      <td>112.0</td>\n",
              "      <td>113.0</td>\n",
              "      <td>112.0</td>\n",
              "      <td>112.0</td>\n",
              "      <td>114.0</td>\n",
              "      <td>2.0</td>\n",
              "      <td>2.0</td>\n",
              "      <td>8.0</td>\n",
              "      <td>10.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>-0.5</td>\n",
              "      <td>-2.0</td>\n",
              "      <td>-4.5</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.5</td>\n",
              "      <td>-2.0</td>\n",
              "      <td>-2.5</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.707107</td>\n",
              "      <td>2.0</td>\n",
              "      <td>4.609772</td>\n",
              "      <td>556684</td>\n",
              "      <td>50.0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>2 rows × 147 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "               LOC          TIME  ...  dist_to_coast  terrain_type_meps\n",
              "0  51.800N 30.250E  1.504872e+09  ...         556684               50.0\n",
              "1  51.800N 30.250E  1.504958e+09  ...         556684               50.0\n",
              "\n",
              "[2 rows x 147 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mHRgeNnoBCj6",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Removing the NAs\n",
        "df = df.dropna(axis = 0)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sFkuhvpABcC5",
        "colab_type": "code",
        "outputId": "790074ff-f3a0-4403-8599-5d8f7ae099ce",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "print(df.shape, dfsaved.shape)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(127751, 147) (1561383, 147)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AKGLDV7YluPo",
        "colab_type": "text"
      },
      "source": [
        "## Finding the ensemble means and sd"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "y8pydvsIfXBQ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# This is to avoid the SettingWithCopyWarning. I did use the .loc to avoid this warning but it does not help. I checked the copying operation and it is correct\n",
        "pd.set_option('mode.chained_assignment', None)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "guV1HSynhktN",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "col = df.loc[: , \"T0.0\":\"T0.9\"]\n",
        "df['T0mensmean'] = col.mean(axis=1)\n",
        "df['T0menssd']   = col.std(axis= 1)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "C6jUMph4jZV5",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "col = df.loc[: , \"T2.0\":\"T2.9\"]\n",
        "df['T2mensmean'] = col.mean(axis=1)\n",
        "df['T2menssd']   = col.std(axis= 1)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bZvOiDUrlevs",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "col = df.loc[: , \"H2.0\":\"H2.9\"]\n",
        "df['H2mensmean'] = col.mean(axis=1)\n",
        "df['H2menssd']   = col.std(axis= 1)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iohWeehIlejG",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "col = df.loc[: , \"T.L850.0\":\"T.L850.9\"]\n",
        "df['T.L850ensmean'] = col.mean(axis=1)\n",
        "df['T.L850enssd']   = col.std(axis= 1)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "i9ad4Z5smJQS",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "col = df.loc[: , \"T.L925.0\":\"T.L925.9\"]\n",
        "df['T.L925ensmean'] = col.mean(axis=1)\n",
        "df['T.L925enssd']   = col.std(axis= 1)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "on4YphJAmW2C",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "col = df.loc[: , \"WS10.0\":\"WS10.9\"]\n",
        "df['WS10ensmean'] = col.mean(axis=1)\n",
        "df['WS10enssd']   = col.std(axis= 1)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VyffYXWBmmAd",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "col = df.loc[: , \"WD10.0\":\"WD10.9\"]\n",
        "df['WD10ensmean'] = col.mean(axis=1)\n",
        "df['WD10enssd']   = col.std(axis= 1)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xbyPY8QwmvtW",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "col = df.loc[: , \"TCC.0\":\"TCC.9\"]\n",
        "df['TCCensmean'] = col.mean(axis=1)\n",
        "df['TCCenssd']   = col.std(axis= 1)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UNIdJ3WBmmuK",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "col = df.loc[: , \"MCC.0\":\"MCC.9\"]\n",
        "df['MCCensmean'] = col.mean(axis=1)\n",
        "df['MCCenssd']   = col.std(axis= 1)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "O7u1Bj34nrQl",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "col = df.loc[: , \"LCC.0\":\"LCC.9\"]\n",
        "df['LCCensmean'] = col.mean(axis=1)\n",
        "df['LCCenssd']   = col.std(axis= 1)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zIvXDgBqn1Jp",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "col = df.loc[: , \"MSLP.0\":\"MSLP.9\"]\n",
        "df['MSLPensmean'] = col.mean(axis=1)\n",
        "df['MSLPenssd']   = col.std(axis= 1)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JVhiZQlZHqbt",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "df['dELEV'] = (df['SG.0']/9.81) - df['elev']"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xSR4sqhcn_U9",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "df_ens = df[['x','y','TA', 'dELEV','T0mensmean', 'T0menssd','T2mensmean', 'T2menssd', 'H2mensmean', 'H2menssd', 'T.L850ensmean', 'T.L850enssd',  'T.L925ensmean', 'T.L925enssd', 'WS10ensmean', 'WS10enssd', 'WD10ensmean', 'WD10enssd','TCCensmean', 'TCCenssd','MCCensmean', 'MCCenssd','LCCensmean', 'LCCenssd', 'MSLPensmean', 'MSLPenssd', 'RR_6'\t ]]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ydfegL0Wt-WP",
        "colab_type": "code",
        "outputId": "8f8e8bfe-7dcd-469e-eed9-65043e30c4a7",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 132
        }
      },
      "source": [
        "df_ens.head(2)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>x</th>\n",
              "      <th>y</th>\n",
              "      <th>TA</th>\n",
              "      <th>dELEV</th>\n",
              "      <th>T0mensmean</th>\n",
              "      <th>T0menssd</th>\n",
              "      <th>T2mensmean</th>\n",
              "      <th>T2menssd</th>\n",
              "      <th>H2mensmean</th>\n",
              "      <th>H2menssd</th>\n",
              "      <th>T.L850ensmean</th>\n",
              "      <th>T.L850enssd</th>\n",
              "      <th>T.L925ensmean</th>\n",
              "      <th>T.L925enssd</th>\n",
              "      <th>WS10ensmean</th>\n",
              "      <th>WS10enssd</th>\n",
              "      <th>WD10ensmean</th>\n",
              "      <th>WD10enssd</th>\n",
              "      <th>TCCensmean</th>\n",
              "      <th>TCCenssd</th>\n",
              "      <th>MCCensmean</th>\n",
              "      <th>MCCenssd</th>\n",
              "      <th>LCCensmean</th>\n",
              "      <th>LCCenssd</th>\n",
              "      <th>MSLPensmean</th>\n",
              "      <th>MSLPenssd</th>\n",
              "      <th>RR_6</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>1815</th>\n",
              "      <td>-1.007560e+06</td>\n",
              "      <td>-1.161066e+06</td>\n",
              "      <td>7.8</td>\n",
              "      <td>-5.328055</td>\n",
              "      <td>11.300040</td>\n",
              "      <td>0.927599</td>\n",
              "      <td>9.352112</td>\n",
              "      <td>1.115544</td>\n",
              "      <td>0.785707</td>\n",
              "      <td>0.123365</td>\n",
              "      <td>0.9597</td>\n",
              "      <td>1.144322</td>\n",
              "      <td>3.5248</td>\n",
              "      <td>1.141523</td>\n",
              "      <td>3.090687</td>\n",
              "      <td>1.661762</td>\n",
              "      <td>259.984084</td>\n",
              "      <td>130.490535</td>\n",
              "      <td>0.733751</td>\n",
              "      <td>0.318869</td>\n",
              "      <td>0.000509</td>\n",
              "      <td>0.001609</td>\n",
              "      <td>0.727766</td>\n",
              "      <td>0.315559</td>\n",
              "      <td>101098.553125</td>\n",
              "      <td>206.687740</td>\n",
              "      <td>0.6</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1816</th>\n",
              "      <td>-1.007560e+06</td>\n",
              "      <td>-1.161066e+06</td>\n",
              "      <td>6.4</td>\n",
              "      <td>-5.328055</td>\n",
              "      <td>7.160144</td>\n",
              "      <td>1.103599</td>\n",
              "      <td>6.423792</td>\n",
              "      <td>0.770699</td>\n",
              "      <td>0.603044</td>\n",
              "      <td>0.058056</td>\n",
              "      <td>-4.7064</td>\n",
              "      <td>0.377092</td>\n",
              "      <td>-0.1996</td>\n",
              "      <td>0.646065</td>\n",
              "      <td>8.414591</td>\n",
              "      <td>0.436086</td>\n",
              "      <td>326.528162</td>\n",
              "      <td>7.304638</td>\n",
              "      <td>0.088218</td>\n",
              "      <td>0.181389</td>\n",
              "      <td>0.041884</td>\n",
              "      <td>0.132382</td>\n",
              "      <td>0.056668</td>\n",
              "      <td>0.092220</td>\n",
              "      <td>100845.116406</td>\n",
              "      <td>79.749582</td>\n",
              "      <td>0.6</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                 x             y   TA  ...    MSLPensmean   MSLPenssd  RR_6\n",
              "1815 -1.007560e+06 -1.161066e+06  7.8  ...  101098.553125  206.687740   0.6\n",
              "1816 -1.007560e+06 -1.161066e+06  6.4  ...  100845.116406   79.749582   0.6\n",
              "\n",
              "[2 rows x 27 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 25
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IsQ-iqRexNDL",
        "colab_type": "code",
        "outputId": "f0bdd7db-03e7-47db-d22a-3417fdcab8ba",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "df_ens.shape"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(127751, 27)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 26
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XTNudl2blAfg",
        "colab_type": "code",
        "outputId": "e198c8a9-ae50-4c82-a929-394e2ff90831",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 139
        }
      },
      "source": [
        "df_ens.columns"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Index(['x', 'y', 'TA', 'dELEV', 'T0mensmean', 'T0menssd', 'T2mensmean',\n",
              "       'T2menssd', 'H2mensmean', 'H2menssd', 'T.L850ensmean', 'T.L850enssd',\n",
              "       'T.L925ensmean', 'T.L925enssd', 'WS10ensmean', 'WS10enssd',\n",
              "       'WD10ensmean', 'WD10enssd', 'TCCensmean', 'TCCenssd', 'MCCensmean',\n",
              "       'MCCenssd', 'LCCensmean', 'LCCenssd', 'MSLPensmean', 'MSLPenssd',\n",
              "       'RR_6'],\n",
              "      dtype='object')"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 27
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "u_03Pt-iq99L",
        "colab_type": "text"
      },
      "source": [
        "## CRPS loss function from RASP paper"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JUUm2qphYVQ2",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Imports\n",
        "import sys\n",
        "\n",
        "import keras\n",
        "from keras.layers import Input, Dense, merge, Embedding, Flatten, Dropout, \\\n",
        "    SimpleRNN, LSTM, TimeDistributed, GRU, Dropout, Masking\n",
        "from keras.layers.merge import Concatenate\n",
        "from keras.models import Model, Sequential\n",
        "import keras.backend as K\n",
        "from keras.callbacks import EarlyStopping\n",
        "from keras.optimizers import SGD, Adam"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "53cBX1FeikRl",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\"\"\"\n",
        "Definition of CRPS loss function.\n",
        "\"\"\"\n",
        "import keras\n",
        "import keras.backend as K\n",
        "import numpy as np\n",
        "# Import erf depending on whether we use the theano or tensorflow backend\n",
        "if keras.backend.backend() == 'tensorflow':\n",
        "  from tensorflow import erf\n",
        "else:\n",
        "  from theano.tensor import erf\n",
        "\n",
        "\n",
        "def crps_cost_function(y_true, y_pred, theano=False):\n",
        "    \"\"\"Compute the CRPS cost function for a normal distribution defined by\n",
        "    the mean and standard deviation.\n",
        "    Code inspired by Kai Polsterer (HITS).\n",
        "    Args:\n",
        "        y_true: True values\n",
        "        y_pred: Tensor containing predictions: [mean, std]\n",
        "        theano: Set to true if using this with pure theano.\n",
        "    Returns:\n",
        "        mean_crps: Scalar with mean CRPS over batch\n",
        "  \"\"\"\n",
        "    # Split input\n",
        "    mu = y_pred[:, 0]\n",
        "    sigma = y_pred[:, 1]\n",
        "    # Ugly workaround for different tensor allocation in keras and theano\n",
        "    if not theano:\n",
        "        y_true = y_true[:, 0]   # Need to also get rid of axis 1 to match!\n",
        "\n",
        "    # To stop sigma from becoming negative we first have to \n",
        "    # convert it the the variance and then take the square\n",
        "    # root again. \n",
        "    var = K.square(sigma)\n",
        "    # The following three variables are just for convenience\n",
        "    loc = (y_true - mu) / K.sqrt(var)\n",
        "    phi = 1.0 / np.sqrt(2.0 * np.pi) * K.exp(-K.square(loc) / 2.0)\n",
        "    Phi = 0.5 * (1.0 + erf(loc / np.sqrt(2.0)))\n",
        "    # First we will compute the crps for each input/target pairtrdata_00+036\n",
        "    crps =  K.sqrt(var) * (loc * (2. * Phi - 1.) + 2 * phi - 1. / np.sqrt(np.pi))\n",
        "    # Then we take the mean. The cost is now a scalartrdata_00+036\n",
        "    return K.mean(crps)\n",
        "\n",
        "\n",
        "def crps_cost_function_seq(y_true, y_pred):\n",
        "    \"\"\"Version of CRPS const function for sequence predictions.\n",
        "    Here the input tensors have dimensions [sample, time_step].\n",
        "    The output has the same dimensions so that keras can apply weights\n",
        "    afterwards for missing data.vo\n",
        "    Args:  \n",
        "        y_true: True values with dimensions [sample, time_step, 1]\n",
        "        y_pred: Predictions with dimensions [sample, time_step, [mean, std]]\n",
        "    Returns:\n",
        "        crps: CRPS with dimensions [sample, time_step]\n",
        "    \"\"\"\n",
        "    # Split input\n",
        "    mu = y_pred[:, :, 0]\n",
        "    sigma = y_pred[:, :, 1]\n",
        "    \n",
        "    tar = y_true[:, :, 0]\n",
        "    # [sample, time_step]\n",
        "\n",
        "    # To stop sigma from becoming negative we first have to \n",
        "    # convert it the the variance and then take the square9.4 \t\n",
        "    # root again. \n",
        "    var = K.square(sigma)\n",
        "    # The following three variables are just for convenience\n",
        "    loc = (tar - mu) / K.sqrt(var)\n",
        "    phi = 1.0 / np.sqrt(2.0 * np.pi) * K.exp(-K.square(loc) / 2.0)\n",
        "    Phi = 0.5 * (1.0 + erf(loc / np.sqrt(2.0)))\n",
        "    # First we will compute the crps for each input/target pair\n",
        "    crps = K.sqrt(var) * (loc * (2. * Phi - 1.) + 2 * phi - 1. / np.sqrt(np.pi))\n",
        "\n",
        "    # Here we do not take the mean because we want keras to be able to apply\n",
        "    # weights afterwards!\n",
        "    return crps\n",
        "\n",
        "\n",
        "def approx_crps_cat(bin_width): \t\n",
        "    \"\"\"Wrapper to pass bin_width as an argument to the loss function.\n",
        "    Args:\n",
        "        bin_width: width of categorical bins\n",
        "    Returns:\n",
        "        loss_function: approximate crps_loss function with bin_width specified\n",
        "    \"\"\" \n",
        "    \n",
        "    def loss(y_true, y_pred):\n",
        "        \"\"\"Approximate CRPS function for categorical output.\n",
        "        Args:\n",
        "            y_true: One-hot-encoded output\n",
        "            y_pred: Probability for each bin\n",
        "        Returns:\n",
        "            approx_crps: Approximate mean CRPS value for batch\n",
        "        \"\"\"\n",
        "        # [sample, cat]\n",
        "        cum_obs = K.cumsum(y_true, axis=1)\n",
        "        cum_preds = K.cumsum(y_pred, axis=1)\n",
        "        approx_crps = K.sum(K.square(cum_obs - cum_preds), axis=1) * bin_width\n",
        "        return K.mean(approx_crps)\n",
        "    return loss"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "prbv3aLeYbVb",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def crps_normal(mu, sigma, y):\n",
        "    \"\"\"\n",
        "    Compute CRPS for a Gaussian distribution. \n",
        "    \"\"\"\n",
        "    loc = (y - mu) / sigma\n",
        "    crps = sigma * (loc * (2 * norm.cdf(loc) - 1) + \n",
        "                    2 * norm.pdf(loc) - 1. / np.sqrt(np.pi))\n",
        "    return crps\n",
        "\n",
        "\n",
        "def maybe_correct_cat_crps(preds, targets, bin_edges):\n",
        "    \"\"\"CRPS for categorical predictions. I think this is correct now.\n",
        "    \"\"\"\n",
        "    # pdb.set_trace()\n",
        "    # Convert input arrays\n",
        "    preds = np.array(np.atleast_2d(preds), dtype='float')\n",
        "    targets = np.array(np.atleast_1d(targets), dtype='float')\n",
        "\n",
        "    # preds [sample, bins]\n",
        "    # Find insert index\n",
        "    mat_bins = np.repeat(np.atleast_2d(bin_edges), targets.shape[0], axis=0)\n",
        "    b = mat_bins.T - targets\n",
        "    b[b < 0] = 999\n",
        "    insert_idxs = np.argmin(b, axis=0)\n",
        "\n",
        "    # Insert\n",
        "    ins_bin_edges = np.array([np.insert(np.array(bin_edges, dtype=float),\n",
        "                                        insert_idxs[i], targets[i])\n",
        "                              for i in range(targets.shape[0])])\n",
        "    ins_preds = np.array(\n",
        "        [np.insert(preds[i], insert_idxs[i], preds[i, insert_idxs[i] - 1])\n",
        "         for i in range(targets.shape[0])])\n",
        "\n",
        "    # Get obs\n",
        "    bin_obs = np.array([(ins_bin_edges[i, :-1] <= targets[i]) &\n",
        "                        (ins_bin_edges[i, 1:] > targets[i])\n",
        "                        for i in range(targets.shape[0])], dtype=int)\n",
        "\n",
        "    # Cumsum with weights\n",
        "    ins_preds *= np.diff(ins_bin_edges, axis=1)\n",
        "    cum_bin_obs = np.cumsum(bin_obs, axis=1)\n",
        "    cum_probs = np.cumsum(ins_preds, axis=1)\n",
        "    cum_probs = (cum_probs.T / cum_probs[:, -1]).T\n",
        "\n",
        "    # Get adjusted preds\n",
        "    adj_cum_probs = np.concatenate((np.zeros((cum_probs.shape[0], 1)),\n",
        "                                    cum_probs), axis=1)\n",
        "    # Compute squared area for each bin\n",
        "    sq_list = []\n",
        "    for i in range(cum_bin_obs.shape[1]):\n",
        "        x_l = np.abs(cum_bin_obs[:, i] - adj_cum_probs[:, i])\n",
        "        x_r = np.abs(cum_bin_obs[:, i] - adj_cum_probs[:, i + 1])\n",
        "        sq = 1./3. * (x_l ** 2 + x_l * x_r + x_r ** 2)\n",
        "        sq_list.append(sq)\n",
        "\n",
        "    # Compute CRPS\n",
        "    crps = np.sum(np.array(sq_list).T * np.diff(ins_bin_edges, axis=1), axis=1)\n",
        "    return np.mean(crps)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "On5l3CeZLBai",
        "colab_type": "text"
      },
      "source": [
        "## CRPS for Raw ensemble values of T2mensmean and . CRPS measures the difference between observed CDF and the forecast CDF. Perfect score is 0"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6QloTBMarDI6",
        "colab_type": "code",
        "outputId": "c57f578d-b3ab-41f2-8680-d49eae228640",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "mu = np.mean(df_ens['T2mensmean'])\n",
        "sigma = np.std(df_ens['T2menssd'])\n",
        "(np.mean(df_ens['T2mensmean']), np.std(df_ens['T2menssd']))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(7.992608128286304, 0.5420754993993724)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 31
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MRsg9WilrD2x",
        "colab_type": "code",
        "outputId": "1b1553cb-b5e6-4912-e39f-74ea1868e109",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "from scipy.stats import norm\n",
        "crps_raw = np.mean(crps_normal(df_ens['T2mensmean'], df_ens['T2menssd'], df['TA']))\n",
        "crps_raw"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "1.1178095637935477"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 32
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "M8Qi7wB3MVoQ",
        "colab_type": "text"
      },
      "source": [
        "## Postprocessing of T2m ensemble mean and standardard deviation with  MEPS T2m and other variables "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IbPYdaDYRsSD",
        "colab_type": "code",
        "outputId": "31f05582-c11c-4772-f065-4f436a1cf2fa",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "#Splitting, Scaling and standardindization\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "train, test = train_test_split(df_ens, test_size=0.3)\n",
        "print(train.shape, test.shape)\n",
        "train_X = train.drop('TA',1)\n",
        "train_y = train[['TA']]\n",
        "train_X.head(2)\n",
        "train_y.head(2)\n",
        "\n",
        "test_X = test.drop('TA',1)\n",
        "test_y = test[['TA']]\n",
        "test_X.head(2)\n",
        "test_y.head(2)\n",
        "\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "# create scaler\n",
        "scaler = StandardScaler()\n",
        "# fit scaler on train data\n",
        "scaler.fit(train_X)\n",
        "# apply transform\n",
        "train_standardized_X  = scaler.transform(train_X)\n",
        "# fit scaler on test data\n",
        "scaler.fit(test_X)\n",
        "# apply transform\n",
        "test_standardized_X  = scaler.transform(test_X)\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(89425, 27) (38326, 27)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "njvwjnkiM-0W",
        "colab_type": "code",
        "outputId": "14ccdeb2-b214-40c5-9c8a-11156fdbbc8a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 243
        }
      },
      "source": [
        "train_standardized_X"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[ 0.34332876,  1.27517011, -0.56896703, ..., -2.0976105 ,\n",
              "        -0.12807282,  0.07765975],\n",
              "       [-0.01926127,  0.13033618, -2.12983235, ...,  0.90190045,\n",
              "        -0.78326048, -0.16653656],\n",
              "       [-0.5441884 ,  0.29363411,  0.84451599, ..., -1.51272548,\n",
              "         0.89003075, -0.16653656],\n",
              "       ...,\n",
              "       [ 2.40133734,  2.11219285, -0.29144261, ...,  0.25084158,\n",
              "        -0.78181081, -0.16653656],\n",
              "       [-0.39980369,  0.64160727, -4.33418853, ..., -1.07171771,\n",
              "         2.34730212,  0.02882049],\n",
              "       [-0.01926127,  0.13033618, -2.12983235, ...,  2.06835308,\n",
              "         0.19721996, -0.16653656]])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 35
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Bm7pczh1WA-e",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QZNgAIQcurFz",
        "colab_type": "code",
        "outputId": "0675120b-d040-40d8-a7db-cf2e4e58d498",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 112
        }
      },
      "source": [
        "train_y.head(2)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>TA</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>1295884</th>\n",
              "      <td>3.1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>736497</th>\n",
              "      <td>21.9</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "           TA\n",
              "1295884   3.1\n",
              "736497   21.9"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 36
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4f8KHKoVyU_7",
        "colab_type": "code",
        "outputId": "acbe374f-b951-4bd2-eadf-3c9ff2da90a6",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 132
        }
      },
      "source": [
        "#Standardized dataframes ensemble mean and deviation all ensemble members\n",
        "# Splitting and scaling gives an array. In this cell we get back the array to a panda dataframe and assign the variable nemaes to the columns\n",
        "\n",
        "train_std_df_X = pd.DataFrame(train_standardized_X)\n",
        "test_std_df_X = pd.DataFrame(test_standardized_X)\n",
        "train_std_df_X.head(2)\n",
        "\n",
        "train_std_df_X.columns = ['x', 'y', 'dELEV', 'T0mensmean', 'T0menssd', 'T2mensmean',\n",
        "       'T2menssd', 'H2mensmean', 'H2menssd', 'T.L850ensmean', 'T.L850enssd',\n",
        "       'T.L925ensmean', 'T.L925enssd', 'WS10ensmean', 'WS10enssd',\n",
        "       'WD10ensmean', 'WD10enssd', 'TCCensmean', 'TCCenssd', 'MCCensmean',\n",
        "       'MCCenssd', 'LCCensmean', 'LCCenssd', 'MSLPensmean', 'MSLPenssd','RR_6']\n",
        "\n",
        "\n",
        "\n",
        "test_std_df_X.columns =['x', 'y', 'dELEV', 'T0mensmean', 'T0menssd', 'T2mensmean',\n",
        "       'T2menssd', 'H2mensmean', 'H2menssd', 'T.L850ensmean', 'T.L850enssd',\n",
        "       'T.L925ensmean', 'T.L925enssd', 'WS10ensmean', 'WS10enssd',\n",
        "       'WD10ensmean', 'WD10enssd', 'TCCensmean', 'TCCenssd', 'MCCensmean',\n",
        "       'MCCenssd', 'LCCensmean', 'LCCenssd', 'MSLPensmean', 'MSLPenssd','RR_6']\n",
        "\n",
        "\n",
        "test_std_df_X.head(2)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>x</th>\n",
              "      <th>y</th>\n",
              "      <th>dELEV</th>\n",
              "      <th>T0mensmean</th>\n",
              "      <th>T0menssd</th>\n",
              "      <th>T2mensmean</th>\n",
              "      <th>T2menssd</th>\n",
              "      <th>H2mensmean</th>\n",
              "      <th>H2menssd</th>\n",
              "      <th>T.L850ensmean</th>\n",
              "      <th>T.L850enssd</th>\n",
              "      <th>T.L925ensmean</th>\n",
              "      <th>T.L925enssd</th>\n",
              "      <th>WS10ensmean</th>\n",
              "      <th>WS10enssd</th>\n",
              "      <th>WD10ensmean</th>\n",
              "      <th>WD10enssd</th>\n",
              "      <th>TCCensmean</th>\n",
              "      <th>TCCenssd</th>\n",
              "      <th>MCCensmean</th>\n",
              "      <th>MCCenssd</th>\n",
              "      <th>LCCensmean</th>\n",
              "      <th>LCCenssd</th>\n",
              "      <th>MSLPensmean</th>\n",
              "      <th>MSLPenssd</th>\n",
              "      <th>RR_6</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>-0.169765</td>\n",
              "      <td>0.838927</td>\n",
              "      <td>-0.308209</td>\n",
              "      <td>-0.436904</td>\n",
              "      <td>-0.814901</td>\n",
              "      <td>-0.860178</td>\n",
              "      <td>-0.482715</td>\n",
              "      <td>0.685359</td>\n",
              "      <td>-0.425404</td>\n",
              "      <td>-1.302246</td>\n",
              "      <td>-1.100846</td>\n",
              "      <td>-1.125684</td>\n",
              "      <td>0.196376</td>\n",
              "      <td>-0.968523</td>\n",
              "      <td>-0.035907</td>\n",
              "      <td>-0.787628</td>\n",
              "      <td>2.221157</td>\n",
              "      <td>0.167491</td>\n",
              "      <td>0.956028</td>\n",
              "      <td>-0.770572</td>\n",
              "      <td>-0.624791</td>\n",
              "      <td>-0.625719</td>\n",
              "      <td>0.558462</td>\n",
              "      <td>-1.409439</td>\n",
              "      <td>-0.485802</td>\n",
              "      <td>-0.124791</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>0.275270</td>\n",
              "      <td>-0.790299</td>\n",
              "      <td>-0.412614</td>\n",
              "      <td>0.850730</td>\n",
              "      <td>-0.862163</td>\n",
              "      <td>1.107661</td>\n",
              "      <td>-0.325823</td>\n",
              "      <td>-1.010807</td>\n",
              "      <td>1.374181</td>\n",
              "      <td>0.607746</td>\n",
              "      <td>0.840705</td>\n",
              "      <td>0.767646</td>\n",
              "      <td>-0.834255</td>\n",
              "      <td>0.967129</td>\n",
              "      <td>0.045412</td>\n",
              "      <td>1.213176</td>\n",
              "      <td>-0.623249</td>\n",
              "      <td>-1.471822</td>\n",
              "      <td>1.136355</td>\n",
              "      <td>-0.661989</td>\n",
              "      <td>0.256018</td>\n",
              "      <td>-0.895200</td>\n",
              "      <td>0.736963</td>\n",
              "      <td>-0.060702</td>\n",
              "      <td>-0.066142</td>\n",
              "      <td>-0.124791</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "          x         y     dELEV  ...  MSLPensmean  MSLPenssd      RR_6\n",
              "0 -0.169765  0.838927 -0.308209  ...    -1.409439  -0.485802 -0.124791\n",
              "1  0.275270 -0.790299 -0.412614  ...    -0.060702  -0.066142 -0.124791\n",
              "\n",
              "[2 rows x 26 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 37
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "guCtacJV3N6F",
        "colab_type": "code",
        "outputId": "d7496135-057b-4802-970c-c112bda115df",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "train_std_df_X.shape"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(89425, 26)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 38
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sGn9oxpBDGpW",
        "colab_type": "text"
      },
      "source": [
        "\n",
        "## **Post processing using the Fully connected or the Linear Regression  model and the neural network model**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "u-V7xrjIB2S4",
        "colab_type": "text"
      },
      "source": [
        "## Fully-Connected FC/LR model "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Zu9p80y0jIbN",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#from ppnn paper\n",
        "def build_fc_model(n_features, n_outputs, compile=False, optimizer='adam',\n",
        "                   lr=0.1, loss=crps_cost_function):\n",
        "    \"\"\"Build (and compile) fully connected linear network.\n",
        "\n",
        "    Args:\n",
        "        n_features: Number of features\n",
        "        n_outputs: Number of outputs\n",
        "        compile: If true, compile model\n",
        "        optimizer:  Name of optimizer\n",
        "        lr: learning rate\n",
        "        loss: loss function\n",
        "\n",
        "    Returns:\n",
        "        model: Keras model\n",
        "    \"\"\"\n",
        "\n",
        "    inp = Input(shape=(n_features,))\n",
        "    x = Dense(n_outputs, activation='linear')(inp)\n",
        "    model = Model(inputs=inp, outputs=x)\n",
        "\n",
        "    if compile:\n",
        "        #opt = keras.optimizers.__dict__[optimizer](lr=lr)\n",
        "        model.compile(optimizer='adam', loss=loss)\n",
        "    return model"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SCTG_R9nMb72",
        "colab_type": "code",
        "outputId": "65671d09-f90e-4772-8e71-94e05ca03dc5",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 243
        }
      },
      "source": [
        "input_features =  len(train_standardized_X[0])\n",
        "print(input_features)\n",
        "fc_model =build_fc_model(input_features, 2, compile=True)\n",
        "fc_model.summary()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "26\n",
            "Model: \"model_3\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "input_3 (InputLayer)         (None, 26)                0         \n",
            "_________________________________________________________________\n",
            "dense_4 (Dense)              (None, 2)                 54        \n",
            "=================================================================\n",
            "Total params: 54\n",
            "Trainable params: 54\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "c2q_CY9yMbyC",
        "colab_type": "code",
        "outputId": "d9cc3902-bd52-411d-9a82-ac8988f3901e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "%%time\n",
        "history = fc_model.fit(train_standardized_X, train_y, epochs=500, batch_size=50, validation_split=0.2, verbose=2, shuffle=False)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Train on 71540 samples, validate on 17885 samples\n",
            "Epoch 1/500\n",
            " - 2s - loss: 6.5425 - val_loss: 5.0742\n",
            "Epoch 2/500\n",
            " - 2s - loss: 4.4947 - val_loss: 3.9908\n",
            "Epoch 3/500\n",
            " - 2s - loss: 3.4477 - val_loss: 2.9423\n",
            "Epoch 4/500\n",
            " - 2s - loss: 2.5009 - val_loss: 2.1130\n",
            "Epoch 5/500\n",
            " - 2s - loss: 1.7730 - val_loss: 1.4822\n",
            "Epoch 6/500\n",
            " - 2s - loss: 1.2567 - val_loss: 1.0830\n",
            "Epoch 7/500\n",
            " - 2s - loss: 1.0197 - val_loss: 0.9811\n",
            "Epoch 8/500\n",
            " - 2s - loss: 0.9778 - val_loss: 0.9609\n",
            "Epoch 9/500\n",
            " - 2s - loss: 0.9641 - val_loss: 0.9499\n",
            "Epoch 10/500\n",
            " - 2s - loss: 0.9547 - val_loss: 0.9415\n",
            "Epoch 11/500\n",
            " - 2s - loss: 0.9470 - val_loss: 0.9345\n",
            "Epoch 12/500\n",
            " - 2s - loss: 0.9404 - val_loss: 0.9285\n",
            "Epoch 13/500\n",
            " - 2s - loss: 0.9349 - val_loss: 0.9233\n",
            "Epoch 14/500\n",
            " - 2s - loss: 0.9301 - val_loss: 0.9189\n",
            "Epoch 15/500\n",
            " - 2s - loss: 0.9260 - val_loss: 0.9151\n",
            "Epoch 16/500\n",
            " - 2s - loss: 0.9225 - val_loss: 0.9118\n",
            "Epoch 17/500\n",
            " - 2s - loss: 0.9194 - val_loss: 0.9090\n",
            "Epoch 18/500\n",
            " - 2s - loss: 0.9168 - val_loss: 0.9066\n",
            "Epoch 19/500\n",
            " - 2s - loss: 0.9146 - val_loss: 0.9045\n",
            "Epoch 20/500\n",
            " - 2s - loss: 0.9127 - val_loss: 0.9027\n",
            "Epoch 21/500\n",
            " - 2s - loss: 0.9111 - val_loss: 0.9012\n",
            "Epoch 22/500\n",
            " - 2s - loss: 0.9098 - val_loss: 0.8999\n",
            "Epoch 23/500\n",
            " - 2s - loss: 0.9086 - val_loss: 0.8988\n",
            "Epoch 24/500\n",
            " - 2s - loss: 0.9076 - val_loss: 0.8979\n",
            "Epoch 25/500\n",
            " - 2s - loss: 0.9068 - val_loss: 0.8971\n",
            "Epoch 26/500\n",
            " - 2s - loss: 0.9061 - val_loss: 0.8964\n",
            "Epoch 27/500\n",
            " - 2s - loss: 0.9055 - val_loss: 0.8959\n",
            "Epoch 28/500\n",
            " - 2s - loss: 0.9050 - val_loss: 0.8954\n",
            "Epoch 29/500\n",
            " - 2s - loss: 0.9045 - val_loss: 0.8950\n",
            "Epoch 30/500\n",
            " - 2s - loss: 0.9042 - val_loss: 0.8946\n",
            "Epoch 31/500\n",
            " - 2s - loss: 0.9038 - val_loss: 0.8943\n",
            "Epoch 32/500\n",
            " - 2s - loss: 0.9036 - val_loss: 0.8941\n",
            "Epoch 33/500\n",
            " - 2s - loss: 0.9034 - val_loss: 0.8938\n",
            "Epoch 34/500\n",
            " - 2s - loss: 0.9032 - val_loss: 0.8937\n",
            "Epoch 35/500\n",
            " - 2s - loss: 0.9030 - val_loss: 0.8935\n",
            "Epoch 36/500\n",
            " - 2s - loss: 0.9029 - val_loss: 0.8934\n",
            "Epoch 37/500\n",
            " - 2s - loss: 0.9028 - val_loss: 0.8933\n",
            "Epoch 38/500\n",
            " - 2s - loss: 0.9027 - val_loss: 0.8932\n",
            "Epoch 39/500\n",
            " - 2s - loss: 0.9026 - val_loss: 0.8931\n",
            "Epoch 40/500\n",
            " - 2s - loss: 0.9025 - val_loss: 0.8930\n",
            "Epoch 41/500\n",
            " - 2s - loss: 0.9025 - val_loss: 0.8929\n",
            "Epoch 42/500\n",
            " - 2s - loss: 0.9024 - val_loss: 0.8929\n",
            "Epoch 43/500\n",
            " - 2s - loss: 0.9024 - val_loss: 0.8929\n",
            "Epoch 44/500\n",
            " - 2s - loss: 0.9024 - val_loss: 0.8928\n",
            "Epoch 45/500\n",
            " - 2s - loss: 0.9023 - val_loss: 0.8928\n",
            "Epoch 46/500\n",
            " - 2s - loss: 0.9023 - val_loss: 0.8928\n",
            "Epoch 47/500\n",
            " - 2s - loss: 0.9023 - val_loss: 0.8927\n",
            "Epoch 48/500\n",
            " - 2s - loss: 0.9023 - val_loss: 0.8927\n",
            "Epoch 49/500\n",
            " - 2s - loss: 0.9022 - val_loss: 0.8927\n",
            "Epoch 50/500\n",
            " - 2s - loss: 0.9022 - val_loss: 0.8927\n",
            "Epoch 51/500\n",
            " - 2s - loss: 0.9022 - val_loss: 0.8927\n",
            "Epoch 52/500\n",
            " - 2s - loss: 0.9022 - val_loss: 0.8926\n",
            "Epoch 53/500\n",
            " - 2s - loss: 0.9022 - val_loss: 0.8926\n",
            "Epoch 54/500\n",
            " - 2s - loss: 0.9022 - val_loss: 0.8926\n",
            "Epoch 55/500\n",
            " - 2s - loss: 0.9022 - val_loss: 0.8926\n",
            "Epoch 56/500\n",
            " - 2s - loss: 0.9022 - val_loss: 0.8926\n",
            "Epoch 57/500\n",
            " - 2s - loss: 0.9022 - val_loss: 0.8926\n",
            "Epoch 58/500\n",
            " - 2s - loss: 0.9022 - val_loss: 0.8926\n",
            "Epoch 59/500\n",
            " - 2s - loss: 0.9022 - val_loss: 0.8926\n",
            "Epoch 60/500\n",
            " - 2s - loss: 0.9022 - val_loss: 0.8926\n",
            "Epoch 61/500\n",
            " - 2s - loss: 0.9022 - val_loss: 0.8926\n",
            "Epoch 62/500\n",
            " - 2s - loss: 0.9022 - val_loss: 0.8926\n",
            "Epoch 63/500\n",
            " - 2s - loss: 0.9022 - val_loss: 0.8926\n",
            "Epoch 64/500\n",
            " - 2s - loss: 0.9022 - val_loss: 0.8926\n",
            "Epoch 65/500\n",
            " - 2s - loss: 0.9022 - val_loss: 0.8926\n",
            "Epoch 66/500\n",
            " - 2s - loss: 0.9022 - val_loss: 0.8926\n",
            "Epoch 67/500\n",
            " - 2s - loss: 0.9022 - val_loss: 0.8926\n",
            "Epoch 68/500\n",
            " - 2s - loss: 0.9022 - val_loss: 0.8926\n",
            "Epoch 69/500\n",
            " - 2s - loss: 0.9022 - val_loss: 0.8926\n",
            "Epoch 70/500\n",
            " - 2s - loss: 0.9022 - val_loss: 0.8926\n",
            "Epoch 71/500\n",
            " - 2s - loss: 0.9022 - val_loss: 0.8926\n",
            "Epoch 72/500\n",
            " - 2s - loss: 0.9022 - val_loss: 0.8926\n",
            "Epoch 73/500\n",
            " - 2s - loss: 0.9022 - val_loss: 0.8926\n",
            "Epoch 74/500\n",
            " - 2s - loss: 0.9022 - val_loss: 0.8926\n",
            "Epoch 75/500\n",
            " - 2s - loss: 0.9022 - val_loss: 0.8926\n",
            "Epoch 76/500\n",
            " - 2s - loss: 0.9022 - val_loss: 0.8926\n",
            "Epoch 77/500\n",
            " - 2s - loss: 0.9022 - val_loss: 0.8926\n",
            "Epoch 78/500\n",
            " - 2s - loss: 0.9022 - val_loss: 0.8926\n",
            "Epoch 79/500\n",
            " - 2s - loss: 0.9022 - val_loss: 0.8926\n",
            "Epoch 80/500\n",
            " - 2s - loss: 0.9022 - val_loss: 0.8926\n",
            "Epoch 81/500\n",
            " - 2s - loss: 0.9022 - val_loss: 0.8926\n",
            "Epoch 82/500\n",
            " - 2s - loss: 0.9022 - val_loss: 0.8926\n",
            "Epoch 83/500\n",
            " - 2s - loss: 0.9022 - val_loss: 0.8926\n",
            "Epoch 84/500\n",
            " - 2s - loss: 0.9022 - val_loss: 0.8926\n",
            "Epoch 85/500\n",
            " - 2s - loss: 0.9022 - val_loss: 0.8926\n",
            "Epoch 86/500\n",
            " - 2s - loss: 0.9022 - val_loss: 0.8926\n",
            "Epoch 87/500\n",
            " - 2s - loss: 0.9022 - val_loss: 0.8926\n",
            "Epoch 88/500\n",
            " - 2s - loss: 0.9022 - val_loss: 0.8926\n",
            "Epoch 89/500\n",
            " - 2s - loss: 0.9022 - val_loss: 0.8926\n",
            "Epoch 90/500\n",
            " - 2s - loss: 0.9022 - val_loss: 0.8926\n",
            "Epoch 91/500\n",
            " - 2s - loss: 0.9022 - val_loss: 0.8926\n",
            "Epoch 92/500\n",
            " - 2s - loss: 0.9022 - val_loss: 0.8926\n",
            "Epoch 93/500\n",
            " - 2s - loss: 0.9022 - val_loss: 0.8926\n",
            "Epoch 94/500\n",
            " - 2s - loss: 0.9022 - val_loss: 0.8926\n",
            "Epoch 95/500\n",
            " - 2s - loss: 0.9022 - val_loss: 0.8926\n",
            "Epoch 96/500\n",
            " - 2s - loss: 0.9022 - val_loss: 0.8926\n",
            "Epoch 97/500\n",
            " - 2s - loss: 0.9022 - val_loss: 0.8926\n",
            "Epoch 98/500\n",
            " - 2s - loss: 0.9022 - val_loss: 0.8926\n",
            "Epoch 99/500\n",
            " - 2s - loss: 0.9022 - val_loss: 0.8926\n",
            "Epoch 100/500\n",
            " - 2s - loss: 0.9022 - val_loss: 0.8926\n",
            "Epoch 101/500\n",
            " - 2s - loss: 0.9022 - val_loss: 0.8926\n",
            "Epoch 102/500\n",
            " - 2s - loss: 0.9022 - val_loss: 0.8926\n",
            "Epoch 103/500\n",
            " - 2s - loss: 0.9022 - val_loss: 0.8926\n",
            "Epoch 104/500\n",
            " - 2s - loss: 0.9022 - val_loss: 0.8926\n",
            "Epoch 105/500\n",
            " - 2s - loss: 0.9022 - val_loss: 0.8926\n",
            "Epoch 106/500\n",
            " - 2s - loss: 0.9022 - val_loss: 0.8926\n",
            "Epoch 107/500\n",
            " - 2s - loss: 0.9022 - val_loss: 0.8926\n",
            "Epoch 108/500\n",
            " - 2s - loss: 0.9022 - val_loss: 0.8926\n",
            "Epoch 109/500\n",
            " - 2s - loss: 0.9022 - val_loss: 0.8926\n",
            "Epoch 110/500\n",
            " - 2s - loss: 0.9022 - val_loss: 0.8926\n",
            "Epoch 111/500\n",
            " - 2s - loss: 0.9022 - val_loss: 0.8926\n",
            "Epoch 112/500\n",
            " - 2s - loss: 0.9022 - val_loss: 0.8926\n",
            "Epoch 113/500\n",
            " - 2s - loss: 0.9022 - val_loss: 0.8926\n",
            "Epoch 114/500\n",
            " - 2s - loss: 0.9022 - val_loss: 0.8926\n",
            "Epoch 115/500\n",
            " - 2s - loss: 0.9022 - val_loss: 0.8926\n",
            "Epoch 116/500\n",
            " - 2s - loss: 0.9022 - val_loss: 0.8926\n",
            "Epoch 117/500\n",
            " - 2s - loss: 0.9022 - val_loss: 0.8926\n",
            "Epoch 118/500\n",
            " - 2s - loss: 0.9022 - val_loss: 0.8926\n",
            "Epoch 119/500\n",
            " - 2s - loss: 0.9022 - val_loss: 0.8926\n",
            "Epoch 120/500\n",
            " - 2s - loss: 0.9022 - val_loss: 0.8926\n",
            "Epoch 121/500\n",
            " - 2s - loss: 0.9022 - val_loss: 0.8926\n",
            "Epoch 122/500\n",
            " - 2s - loss: 0.9022 - val_loss: 0.8926\n",
            "Epoch 123/500\n",
            " - 2s - loss: 0.9022 - val_loss: 0.8926\n",
            "Epoch 124/500\n",
            " - 2s - loss: 0.9022 - val_loss: 0.8926\n",
            "Epoch 125/500\n",
            " - 2s - loss: 0.9022 - val_loss: 0.8926\n",
            "Epoch 126/500\n",
            " - 2s - loss: 0.9022 - val_loss: 0.8926\n",
            "Epoch 127/500\n",
            " - 2s - loss: 0.9022 - val_loss: 0.8926\n",
            "Epoch 128/500\n",
            " - 2s - loss: 0.9022 - val_loss: 0.8926\n",
            "Epoch 129/500\n",
            " - 2s - loss: 0.9022 - val_loss: 0.8926\n",
            "Epoch 130/500\n",
            " - 2s - loss: 0.9022 - val_loss: 0.8926\n",
            "Epoch 131/500\n",
            " - 2s - loss: 0.9022 - val_loss: 0.8926\n",
            "Epoch 132/500\n",
            " - 2s - loss: 0.9022 - val_loss: 0.8926\n",
            "Epoch 133/500\n",
            " - 2s - loss: 0.9022 - val_loss: 0.8926\n",
            "Epoch 134/500\n",
            " - 2s - loss: 0.9022 - val_loss: 0.8926\n",
            "Epoch 135/500\n",
            " - 2s - loss: 0.9022 - val_loss: 0.8926\n",
            "Epoch 136/500\n",
            " - 2s - loss: 0.9022 - val_loss: 0.8926\n",
            "Epoch 137/500\n",
            " - 2s - loss: 0.9022 - val_loss: 0.8926\n",
            "Epoch 138/500\n",
            " - 2s - loss: 0.9022 - val_loss: 0.8926\n",
            "Epoch 139/500\n",
            " - 2s - loss: 0.9022 - val_loss: 0.8926\n",
            "Epoch 140/500\n",
            " - 2s - loss: 0.9022 - val_loss: 0.8926\n",
            "Epoch 141/500\n",
            " - 2s - loss: 0.9022 - val_loss: 0.8926\n",
            "Epoch 142/500\n",
            " - 2s - loss: 0.9022 - val_loss: 0.8926\n",
            "Epoch 143/500\n",
            " - 2s - loss: 0.9022 - val_loss: 0.8926\n",
            "Epoch 144/500\n",
            " - 2s - loss: 0.9022 - val_loss: 0.8926\n",
            "Epoch 145/500\n",
            " - 2s - loss: 0.9022 - val_loss: 0.8926\n",
            "Epoch 146/500\n",
            " - 2s - loss: 0.9022 - val_loss: 0.8926\n",
            "Epoch 147/500\n",
            " - 2s - loss: 0.9022 - val_loss: 0.8926\n",
            "Epoch 148/500\n",
            " - 2s - loss: 0.9022 - val_loss: 0.8926\n",
            "Epoch 149/500\n",
            " - 2s - loss: 0.9022 - val_loss: 0.8926\n",
            "Epoch 150/500\n",
            " - 2s - loss: 0.9022 - val_loss: 0.8926\n",
            "Epoch 151/500\n",
            " - 2s - loss: 0.9022 - val_loss: 0.8926\n",
            "Epoch 152/500\n",
            " - 2s - loss: 0.9022 - val_loss: 0.8926\n",
            "Epoch 153/500\n",
            " - 2s - loss: 0.9022 - val_loss: 0.8926\n",
            "Epoch 154/500\n",
            " - 2s - loss: 0.9022 - val_loss: 0.8926\n",
            "Epoch 155/500\n",
            " - 2s - loss: 0.9022 - val_loss: 0.8926\n",
            "Epoch 156/500\n",
            " - 2s - loss: 0.9022 - val_loss: 0.8926\n",
            "Epoch 157/500\n",
            " - 2s - loss: 0.9022 - val_loss: 0.8926\n",
            "Epoch 158/500\n",
            " - 2s - loss: 0.9022 - val_loss: 0.8926\n",
            "Epoch 159/500\n",
            " - 2s - loss: 0.9022 - val_loss: 0.8926\n",
            "Epoch 160/500\n",
            " - 2s - loss: 0.9022 - val_loss: 0.8926\n",
            "Epoch 161/500\n",
            " - 2s - loss: 0.9022 - val_loss: 0.8926\n",
            "Epoch 162/500\n",
            " - 2s - loss: 0.9022 - val_loss: 0.8926\n",
            "Epoch 163/500\n",
            " - 2s - loss: 0.9022 - val_loss: 0.8926\n",
            "Epoch 164/500\n",
            " - 2s - loss: 0.9022 - val_loss: 0.8926\n",
            "Epoch 165/500\n",
            " - 2s - loss: 0.9022 - val_loss: 0.8926\n",
            "Epoch 166/500\n",
            " - 2s - loss: 0.9022 - val_loss: 0.8926\n",
            "Epoch 167/500\n",
            " - 2s - loss: 0.9022 - val_loss: 0.8926\n",
            "Epoch 168/500\n",
            " - 2s - loss: 0.9022 - val_loss: 0.8926\n",
            "Epoch 169/500\n",
            " - 2s - loss: 0.9022 - val_loss: 0.8926\n",
            "Epoch 170/500\n",
            " - 2s - loss: 0.9022 - val_loss: 0.8926\n",
            "Epoch 171/500\n",
            " - 2s - loss: 0.9022 - val_loss: 0.8926\n",
            "Epoch 172/500\n",
            " - 2s - loss: 0.9022 - val_loss: 0.8926\n",
            "Epoch 173/500\n",
            " - 2s - loss: 0.9022 - val_loss: 0.8926\n",
            "Epoch 174/500\n",
            " - 2s - loss: 0.9022 - val_loss: 0.8926\n",
            "Epoch 175/500\n",
            " - 2s - loss: 0.9022 - val_loss: 0.8926\n",
            "Epoch 176/500\n",
            " - 2s - loss: 0.9022 - val_loss: 0.8926\n",
            "Epoch 177/500\n",
            " - 2s - loss: 0.9022 - val_loss: 0.8926\n",
            "Epoch 178/500\n",
            " - 2s - loss: 0.9022 - val_loss: 0.8926\n",
            "Epoch 179/500\n",
            " - 2s - loss: 0.9022 - val_loss: 0.8926\n",
            "Epoch 180/500\n",
            " - 2s - loss: 0.9022 - val_loss: 0.8926\n",
            "Epoch 181/500\n",
            " - 2s - loss: 0.9022 - val_loss: 0.8926\n",
            "Epoch 182/500\n",
            " - 2s - loss: 0.9022 - val_loss: 0.8926\n",
            "Epoch 183/500\n",
            " - 2s - loss: 0.9022 - val_loss: 0.8926\n",
            "Epoch 184/500\n",
            " - 2s - loss: 0.9022 - val_loss: 0.8926\n",
            "Epoch 185/500\n",
            " - 2s - loss: 0.9022 - val_loss: 0.8926\n",
            "Epoch 186/500\n",
            " - 2s - loss: 0.9022 - val_loss: 0.8926\n",
            "Epoch 187/500\n",
            " - 2s - loss: 0.9022 - val_loss: 0.8926\n",
            "Epoch 188/500\n",
            " - 2s - loss: 0.9022 - val_loss: 0.8926\n",
            "Epoch 189/500\n",
            " - 2s - loss: 0.9022 - val_loss: 0.8926\n",
            "Epoch 190/500\n",
            " - 2s - loss: 0.9022 - val_loss: 0.8926\n",
            "Epoch 191/500\n",
            " - 2s - loss: 0.9022 - val_loss: 0.8926\n",
            "Epoch 192/500\n",
            " - 2s - loss: 0.9022 - val_loss: 0.8926\n",
            "Epoch 193/500\n",
            " - 2s - loss: 0.9022 - val_loss: 0.8926\n",
            "Epoch 194/500\n",
            " - 2s - loss: 0.9022 - val_loss: 0.8926\n",
            "Epoch 195/500\n",
            " - 2s - loss: 0.9022 - val_loss: 0.8926\n",
            "Epoch 196/500\n",
            " - 2s - loss: 0.9022 - val_loss: 0.8926\n",
            "Epoch 197/500\n",
            " - 2s - loss: 0.9022 - val_loss: 0.8926\n",
            "Epoch 198/500\n",
            " - 2s - loss: 0.9022 - val_loss: 0.8926\n",
            "Epoch 199/500\n",
            " - 2s - loss: 0.9022 - val_loss: 0.8926\n",
            "Epoch 200/500\n",
            " - 2s - loss: 0.9022 - val_loss: 0.8926\n",
            "Epoch 201/500\n",
            " - 2s - loss: 0.9022 - val_loss: 0.8926\n",
            "Epoch 202/500\n",
            " - 2s - loss: 0.9022 - val_loss: 0.8926\n",
            "Epoch 203/500\n",
            " - 2s - loss: 0.9022 - val_loss: 0.8926\n",
            "Epoch 204/500\n",
            " - 2s - loss: 0.9022 - val_loss: 0.8926\n",
            "Epoch 205/500\n",
            " - 2s - loss: 0.9022 - val_loss: 0.8926\n",
            "Epoch 206/500\n",
            " - 2s - loss: 0.9022 - val_loss: 0.8926\n",
            "Epoch 207/500\n",
            " - 2s - loss: 0.9022 - val_loss: 0.8926\n",
            "Epoch 208/500\n",
            " - 2s - loss: 0.9022 - val_loss: 0.8926\n",
            "Epoch 209/500\n",
            " - 2s - loss: 0.9022 - val_loss: 0.8926\n",
            "Epoch 210/500\n",
            " - 2s - loss: 0.9022 - val_loss: 0.8926\n",
            "Epoch 211/500\n",
            " - 2s - loss: 0.9022 - val_loss: 0.8926\n",
            "Epoch 212/500\n",
            " - 2s - loss: 0.9022 - val_loss: 0.8926\n",
            "Epoch 213/500\n",
            " - 2s - loss: 0.9022 - val_loss: 0.8926\n",
            "Epoch 214/500\n",
            " - 2s - loss: 0.9022 - val_loss: 0.8926\n",
            "Epoch 215/500\n",
            " - 2s - loss: 0.9022 - val_loss: 0.8926\n",
            "Epoch 216/500\n",
            " - 2s - loss: 0.9022 - val_loss: 0.8926\n",
            "Epoch 217/500\n",
            " - 2s - loss: 0.9022 - val_loss: 0.8926\n",
            "Epoch 218/500\n",
            " - 2s - loss: 0.9022 - val_loss: 0.8926\n",
            "Epoch 219/500\n",
            " - 2s - loss: 0.9022 - val_loss: 0.8926\n",
            "Epoch 220/500\n",
            " - 2s - loss: 0.9022 - val_loss: 0.8926\n",
            "Epoch 221/500\n",
            " - 2s - loss: 0.9022 - val_loss: 0.8926\n",
            "Epoch 222/500\n",
            " - 2s - loss: 0.9022 - val_loss: 0.8926\n",
            "Epoch 223/500\n",
            " - 2s - loss: 0.9022 - val_loss: 0.8926\n",
            "Epoch 224/500\n",
            " - 2s - loss: 0.9022 - val_loss: 0.8926\n",
            "Epoch 225/500\n",
            " - 2s - loss: 0.9022 - val_loss: 0.8926\n",
            "Epoch 226/500\n",
            " - 2s - loss: 0.9022 - val_loss: 0.8926\n",
            "Epoch 227/500\n",
            " - 2s - loss: 0.9022 - val_loss: 0.8926\n",
            "Epoch 228/500\n",
            " - 2s - loss: 0.9022 - val_loss: 0.8926\n",
            "Epoch 229/500\n",
            " - 2s - loss: 0.9022 - val_loss: 0.8926\n",
            "Epoch 230/500\n",
            " - 2s - loss: 0.9022 - val_loss: 0.8926\n",
            "Epoch 231/500\n",
            " - 2s - loss: 0.9022 - val_loss: 0.8926\n",
            "Epoch 232/500\n",
            " - 2s - loss: 0.9022 - val_loss: 0.8926\n",
            "Epoch 233/500\n",
            " - 2s - loss: 0.9022 - val_loss: 0.8926\n",
            "Epoch 234/500\n",
            " - 2s - loss: 0.9022 - val_loss: 0.8926\n",
            "Epoch 235/500\n",
            " - 2s - loss: 0.9022 - val_loss: 0.8926\n",
            "Epoch 236/500\n",
            " - 2s - loss: 0.9022 - val_loss: 0.8926\n",
            "Epoch 237/500\n",
            " - 2s - loss: 0.9022 - val_loss: 0.8926\n",
            "Epoch 238/500\n",
            " - 2s - loss: 0.9022 - val_loss: 0.8926\n",
            "Epoch 239/500\n",
            " - 2s - loss: 0.9022 - val_loss: 0.8926\n",
            "Epoch 240/500\n",
            " - 2s - loss: 0.9022 - val_loss: 0.8926\n",
            "Epoch 241/500\n",
            " - 2s - loss: 0.9022 - val_loss: 0.8926\n",
            "Epoch 242/500\n",
            " - 2s - loss: 0.9022 - val_loss: 0.8926\n",
            "Epoch 243/500\n",
            " - 2s - loss: 0.9022 - val_loss: 0.8926\n",
            "Epoch 244/500\n",
            " - 2s - loss: 0.9022 - val_loss: 0.8926\n",
            "Epoch 245/500\n",
            " - 2s - loss: 0.9022 - val_loss: 0.8926\n",
            "Epoch 246/500\n",
            " - 2s - loss: 0.9022 - val_loss: 0.8926\n",
            "Epoch 247/500\n",
            " - 2s - loss: 0.9022 - val_loss: 0.8926\n",
            "Epoch 248/500\n",
            " - 2s - loss: 0.9022 - val_loss: 0.8926\n",
            "Epoch 249/500\n",
            " - 2s - loss: 0.9022 - val_loss: 0.8926\n",
            "Epoch 250/500\n",
            " - 2s - loss: 0.9022 - val_loss: 0.8926\n",
            "Epoch 251/500\n",
            " - 2s - loss: 0.9022 - val_loss: 0.8926\n",
            "Epoch 252/500\n",
            " - 2s - loss: 0.9022 - val_loss: 0.8926\n",
            "Epoch 253/500\n",
            " - 2s - loss: 0.9022 - val_loss: 0.8926\n",
            "Epoch 254/500\n",
            " - 2s - loss: 0.9022 - val_loss: 0.8926\n",
            "Epoch 255/500\n",
            " - 2s - loss: 0.9022 - val_loss: 0.8926\n",
            "Epoch 256/500\n",
            " - 2s - loss: 0.9022 - val_loss: 0.8926\n",
            "Epoch 257/500\n",
            " - 2s - loss: 0.9022 - val_loss: 0.8926\n",
            "Epoch 258/500\n",
            " - 2s - loss: 0.9022 - val_loss: 0.8926\n",
            "Epoch 259/500\n",
            " - 2s - loss: 0.9022 - val_loss: 0.8926\n",
            "Epoch 260/500\n",
            " - 2s - loss: 0.9022 - val_loss: 0.8926\n",
            "Epoch 261/500\n",
            " - 2s - loss: 0.9022 - val_loss: 0.8926\n",
            "Epoch 262/500\n",
            " - 2s - loss: 0.9022 - val_loss: 0.8926\n",
            "Epoch 263/500\n",
            " - 2s - loss: 0.9022 - val_loss: 0.8926\n",
            "Epoch 264/500\n",
            " - 2s - loss: 0.9022 - val_loss: 0.8926\n",
            "Epoch 265/500\n",
            " - 2s - loss: 0.9022 - val_loss: 0.8926\n",
            "Epoch 266/500\n",
            " - 2s - loss: 0.9022 - val_loss: 0.8926\n",
            "Epoch 267/500\n",
            " - 2s - loss: 0.9022 - val_loss: 0.8926\n",
            "Epoch 268/500\n",
            " - 2s - loss: 0.9022 - val_loss: 0.8926\n",
            "Epoch 269/500\n",
            " - 2s - loss: 0.9022 - val_loss: 0.8926\n",
            "Epoch 270/500\n",
            " - 2s - loss: 0.9022 - val_loss: 0.8926\n",
            "Epoch 271/500\n",
            " - 2s - loss: 0.9022 - val_loss: 0.8926\n",
            "Epoch 272/500\n",
            " - 2s - loss: 0.9022 - val_loss: 0.8926\n",
            "Epoch 273/500\n",
            " - 2s - loss: 0.9022 - val_loss: 0.8926\n",
            "Epoch 274/500\n",
            " - 2s - loss: 0.9022 - val_loss: 0.8926\n",
            "Epoch 275/500\n",
            " - 2s - loss: 0.9022 - val_loss: 0.8926\n",
            "Epoch 276/500\n",
            " - 2s - loss: 0.9022 - val_loss: 0.8926\n",
            "Epoch 277/500\n",
            " - 2s - loss: 0.9022 - val_loss: 0.8926\n",
            "Epoch 278/500\n",
            " - 2s - loss: 0.9022 - val_loss: 0.8926\n",
            "Epoch 279/500\n",
            " - 2s - loss: 0.9022 - val_loss: 0.8926\n",
            "Epoch 280/500\n",
            " - 2s - loss: 0.9022 - val_loss: 0.8926\n",
            "Epoch 281/500\n",
            " - 2s - loss: 0.9022 - val_loss: 0.8926\n",
            "Epoch 282/500\n",
            " - 2s - loss: 0.9022 - val_loss: 0.8926\n",
            "Epoch 283/500\n",
            " - 2s - loss: 0.9022 - val_loss: 0.8926\n",
            "Epoch 284/500\n",
            " - 2s - loss: 0.9022 - val_loss: 0.8926\n",
            "Epoch 285/500\n",
            " - 2s - loss: 0.9022 - val_loss: 0.8926\n",
            "Epoch 286/500\n",
            " - 2s - loss: 0.9022 - val_loss: 0.8926\n",
            "Epoch 287/500\n",
            " - 2s - loss: 0.9022 - val_loss: 0.8926\n",
            "Epoch 288/500\n",
            " - 2s - loss: 0.9022 - val_loss: 0.8926\n",
            "Epoch 289/500\n",
            " - 2s - loss: 0.9022 - val_loss: 0.8926\n",
            "Epoch 290/500\n",
            " - 2s - loss: 0.9022 - val_loss: 0.8926\n",
            "Epoch 291/500\n",
            " - 2s - loss: 0.9022 - val_loss: 0.8926\n",
            "Epoch 292/500\n",
            " - 2s - loss: 0.9022 - val_loss: 0.8926\n",
            "Epoch 293/500\n",
            " - 2s - loss: 0.9022 - val_loss: 0.8926\n",
            "Epoch 294/500\n",
            " - 2s - loss: 0.9022 - val_loss: 0.8926\n",
            "Epoch 295/500\n",
            " - 2s - loss: 0.9022 - val_loss: 0.8926\n",
            "Epoch 296/500\n",
            " - 2s - loss: 0.9022 - val_loss: 0.8926\n",
            "Epoch 297/500\n",
            " - 2s - loss: 0.9022 - val_loss: 0.8926\n",
            "Epoch 298/500\n",
            " - 2s - loss: 0.9022 - val_loss: 0.8926\n",
            "Epoch 299/500\n",
            " - 2s - loss: 0.9022 - val_loss: 0.8926\n",
            "Epoch 300/500\n",
            " - 2s - loss: 0.9022 - val_loss: 0.8926\n",
            "Epoch 301/500\n",
            " - 2s - loss: 0.9022 - val_loss: 0.8926\n",
            "Epoch 302/500\n",
            " - 2s - loss: 0.9022 - val_loss: 0.8926\n",
            "Epoch 303/500\n",
            " - 2s - loss: 0.9022 - val_loss: 0.8926\n",
            "Epoch 304/500\n",
            " - 2s - loss: 0.9022 - val_loss: 0.8926\n",
            "Epoch 305/500\n",
            " - 2s - loss: 0.9022 - val_loss: 0.8926\n",
            "Epoch 306/500\n",
            " - 2s - loss: 0.9022 - val_loss: 0.8926\n",
            "Epoch 307/500\n",
            " - 2s - loss: 0.9022 - val_loss: 0.8926\n",
            "Epoch 308/500\n",
            " - 2s - loss: 0.9022 - val_loss: 0.8926\n",
            "Epoch 309/500\n",
            " - 2s - loss: 0.9022 - val_loss: 0.8926\n",
            "Epoch 310/500\n",
            " - 2s - loss: 0.9022 - val_loss: 0.8926\n",
            "Epoch 311/500\n",
            " - 2s - loss: 0.9022 - val_loss: 0.8926\n",
            "Epoch 312/500\n",
            " - 2s - loss: 0.9022 - val_loss: 0.8926\n",
            "Epoch 313/500\n",
            " - 2s - loss: 0.9022 - val_loss: 0.8926\n",
            "Epoch 314/500\n",
            " - 2s - loss: 0.9022 - val_loss: 0.8926\n",
            "Epoch 315/500\n",
            " - 2s - loss: 0.9022 - val_loss: 0.8926\n",
            "Epoch 316/500\n",
            " - 2s - loss: 0.9022 - val_loss: 0.8926\n",
            "Epoch 317/500\n",
            " - 2s - loss: 0.9022 - val_loss: 0.8926\n",
            "Epoch 318/500\n",
            " - 2s - loss: 0.9022 - val_loss: 0.8926\n",
            "Epoch 319/500\n",
            " - 2s - loss: 0.9022 - val_loss: 0.8926\n",
            "Epoch 320/500\n",
            " - 2s - loss: 0.9022 - val_loss: 0.8926\n",
            "Epoch 321/500\n",
            " - 2s - loss: 0.9022 - val_loss: 0.8926\n",
            "Epoch 322/500\n",
            " - 2s - loss: 0.9022 - val_loss: 0.8926\n",
            "Epoch 323/500\n",
            " - 2s - loss: 0.9022 - val_loss: 0.8926\n",
            "Epoch 324/500\n",
            " - 2s - loss: 0.9022 - val_loss: 0.8926\n",
            "Epoch 325/500\n",
            " - 2s - loss: 0.9022 - val_loss: 0.8926\n",
            "Epoch 326/500\n",
            " - 2s - loss: 0.9022 - val_loss: 0.8926\n",
            "Epoch 327/500\n",
            " - 2s - loss: 0.9022 - val_loss: 0.8926\n",
            "Epoch 328/500\n",
            " - 2s - loss: 0.9022 - val_loss: 0.8926\n",
            "Epoch 329/500\n",
            " - 2s - loss: 0.9022 - val_loss: 0.8926\n",
            "Epoch 330/500\n",
            " - 2s - loss: 0.9022 - val_loss: 0.8926\n",
            "Epoch 331/500\n",
            " - 2s - loss: 0.9022 - val_loss: 0.8926\n",
            "Epoch 332/500\n",
            " - 2s - loss: 0.9022 - val_loss: 0.8926\n",
            "Epoch 333/500\n",
            " - 2s - loss: 0.9022 - val_loss: 0.8926\n",
            "Epoch 334/500\n",
            " - 2s - loss: 0.9022 - val_loss: 0.8926\n",
            "Epoch 335/500\n",
            " - 2s - loss: 0.9022 - val_loss: 0.8926\n",
            "Epoch 336/500\n",
            " - 2s - loss: 0.9022 - val_loss: 0.8926\n",
            "Epoch 337/500\n",
            " - 2s - loss: 0.9022 - val_loss: 0.8926\n",
            "Epoch 338/500\n",
            " - 2s - loss: 0.9022 - val_loss: 0.8926\n",
            "Epoch 339/500\n",
            " - 2s - loss: 0.9022 - val_loss: 0.8926\n",
            "Epoch 340/500\n",
            " - 2s - loss: 0.9022 - val_loss: 0.8926\n",
            "Epoch 341/500\n",
            " - 2s - loss: 0.9022 - val_loss: 0.8926\n",
            "Epoch 342/500\n",
            " - 2s - loss: 0.9022 - val_loss: 0.8926\n",
            "Epoch 343/500\n",
            " - 2s - loss: 0.9022 - val_loss: 0.8926\n",
            "Epoch 344/500\n",
            " - 2s - loss: 0.9022 - val_loss: 0.8926\n",
            "Epoch 345/500\n",
            " - 2s - loss: 0.9022 - val_loss: 0.8926\n",
            "Epoch 346/500\n",
            " - 2s - loss: 0.9022 - val_loss: 0.8926\n",
            "Epoch 347/500\n",
            " - 2s - loss: 0.9022 - val_loss: 0.8926\n",
            "Epoch 348/500\n",
            " - 2s - loss: 0.9022 - val_loss: 0.8926\n",
            "Epoch 349/500\n",
            " - 2s - loss: 0.9022 - val_loss: 0.8926\n",
            "Epoch 350/500\n",
            " - 2s - loss: 0.9022 - val_loss: 0.8926\n",
            "Epoch 351/500\n",
            " - 2s - loss: 0.9022 - val_loss: 0.8926\n",
            "Epoch 352/500\n",
            " - 2s - loss: 0.9022 - val_loss: 0.8926\n",
            "Epoch 353/500\n",
            " - 2s - loss: 0.9022 - val_loss: 0.8926\n",
            "Epoch 354/500\n",
            " - 2s - loss: 0.9022 - val_loss: 0.8926\n",
            "Epoch 355/500\n",
            " - 2s - loss: 0.9022 - val_loss: 0.8926\n",
            "Epoch 356/500\n",
            " - 2s - loss: 0.9022 - val_loss: 0.8926\n",
            "Epoch 357/500\n",
            " - 2s - loss: 0.9022 - val_loss: 0.8926\n",
            "Epoch 358/500\n",
            " - 2s - loss: 0.9022 - val_loss: 0.8926\n",
            "Epoch 359/500\n",
            " - 2s - loss: 0.9022 - val_loss: 0.8926\n",
            "Epoch 360/500\n",
            " - 2s - loss: 0.9022 - val_loss: 0.8926\n",
            "Epoch 361/500\n",
            " - 2s - loss: 0.9022 - val_loss: 0.8926\n",
            "Epoch 362/500\n",
            " - 2s - loss: 0.9022 - val_loss: 0.8926\n",
            "Epoch 363/500\n",
            " - 2s - loss: 0.9022 - val_loss: 0.8926\n",
            "Epoch 364/500\n",
            " - 2s - loss: 0.9022 - val_loss: 0.8926\n",
            "Epoch 365/500\n",
            " - 2s - loss: 0.9022 - val_loss: 0.8926\n",
            "Epoch 366/500\n",
            " - 2s - loss: 0.9022 - val_loss: 0.8926\n",
            "Epoch 367/500\n",
            " - 2s - loss: 0.9022 - val_loss: 0.8926\n",
            "Epoch 368/500\n",
            " - 2s - loss: 0.9022 - val_loss: 0.8926\n",
            "Epoch 369/500\n",
            " - 2s - loss: 0.9022 - val_loss: 0.8926\n",
            "Epoch 370/500\n",
            " - 2s - loss: 0.9022 - val_loss: 0.8926\n",
            "Epoch 371/500\n",
            " - 2s - loss: 0.9022 - val_loss: 0.8926\n",
            "Epoch 372/500\n",
            " - 2s - loss: 0.9022 - val_loss: 0.8926\n",
            "Epoch 373/500\n",
            " - 2s - loss: 0.9022 - val_loss: 0.8926\n",
            "Epoch 374/500\n",
            " - 2s - loss: 0.9022 - val_loss: 0.8926\n",
            "Epoch 375/500\n",
            " - 2s - loss: 0.9022 - val_loss: 0.8926\n",
            "Epoch 376/500\n",
            " - 2s - loss: 0.9022 - val_loss: 0.8926\n",
            "Epoch 377/500\n",
            " - 2s - loss: 0.9022 - val_loss: 0.8926\n",
            "Epoch 378/500\n",
            " - 2s - loss: 0.9022 - val_loss: 0.8926\n",
            "Epoch 379/500\n",
            " - 2s - loss: 0.9022 - val_loss: 0.8926\n",
            "Epoch 380/500\n",
            " - 2s - loss: 0.9022 - val_loss: 0.8926\n",
            "Epoch 381/500\n",
            " - 2s - loss: 0.9022 - val_loss: 0.8926\n",
            "Epoch 382/500\n",
            " - 2s - loss: 0.9022 - val_loss: 0.8926\n",
            "Epoch 383/500\n",
            " - 2s - loss: 0.9022 - val_loss: 0.8926\n",
            "Epoch 384/500\n",
            " - 2s - loss: 0.9022 - val_loss: 0.8926\n",
            "Epoch 385/500\n",
            " - 2s - loss: 0.9022 - val_loss: 0.8926\n",
            "Epoch 386/500\n",
            " - 2s - loss: 0.9022 - val_loss: 0.8926\n",
            "Epoch 387/500\n",
            " - 2s - loss: 0.9022 - val_loss: 0.8926\n",
            "Epoch 388/500\n",
            " - 2s - loss: 0.9022 - val_loss: 0.8926\n",
            "Epoch 389/500\n",
            " - 2s - loss: 0.9022 - val_loss: 0.8926\n",
            "Epoch 390/500\n",
            " - 2s - loss: 0.9022 - val_loss: 0.8926\n",
            "Epoch 391/500\n",
            " - 2s - loss: 0.9022 - val_loss: 0.8926\n",
            "Epoch 392/500\n",
            " - 2s - loss: 0.9022 - val_loss: 0.8926\n",
            "Epoch 393/500\n",
            " - 2s - loss: 0.9022 - val_loss: 0.8926\n",
            "Epoch 394/500\n",
            " - 2s - loss: 0.9022 - val_loss: 0.8926\n",
            "Epoch 395/500\n",
            " - 2s - loss: 0.9022 - val_loss: 0.8926\n",
            "Epoch 396/500\n",
            " - 2s - loss: 0.9022 - val_loss: 0.8926\n",
            "Epoch 397/500\n",
            " - 2s - loss: 0.9022 - val_loss: 0.8926\n",
            "Epoch 398/500\n",
            " - 2s - loss: 0.9022 - val_loss: 0.8926\n",
            "Epoch 399/500\n",
            " - 2s - loss: 0.9022 - val_loss: 0.8926\n",
            "Epoch 400/500\n",
            " - 2s - loss: 0.9022 - val_loss: 0.8926\n",
            "Epoch 401/500\n",
            " - 2s - loss: 0.9022 - val_loss: 0.8926\n",
            "Epoch 402/500\n",
            " - 2s - loss: 0.9022 - val_loss: 0.8926\n",
            "Epoch 403/500\n",
            " - 2s - loss: 0.9022 - val_loss: 0.8926\n",
            "Epoch 404/500\n",
            " - 2s - loss: 0.9022 - val_loss: 0.8926\n",
            "Epoch 405/500\n",
            " - 2s - loss: 0.9022 - val_loss: 0.8926\n",
            "Epoch 406/500\n",
            " - 2s - loss: 0.9022 - val_loss: 0.8926\n",
            "Epoch 407/500\n",
            " - 2s - loss: 0.9022 - val_loss: 0.8926\n",
            "Epoch 408/500\n",
            " - 2s - loss: 0.9022 - val_loss: 0.8926\n",
            "Epoch 409/500\n",
            " - 2s - loss: 0.9022 - val_loss: 0.8926\n",
            "Epoch 410/500\n",
            " - 2s - loss: 0.9022 - val_loss: 0.8926\n",
            "Epoch 411/500\n",
            " - 2s - loss: 0.9022 - val_loss: 0.8926\n",
            "Epoch 412/500\n",
            " - 2s - loss: 0.9022 - val_loss: 0.8926\n",
            "Epoch 413/500\n",
            " - 2s - loss: 0.9022 - val_loss: 0.8926\n",
            "Epoch 414/500\n",
            " - 2s - loss: 0.9022 - val_loss: 0.8926\n",
            "Epoch 415/500\n",
            " - 2s - loss: 0.9022 - val_loss: 0.8926\n",
            "Epoch 416/500\n",
            " - 2s - loss: 0.9022 - val_loss: 0.8926\n",
            "Epoch 417/500\n",
            " - 2s - loss: 0.9022 - val_loss: 0.8926\n",
            "Epoch 418/500\n",
            " - 2s - loss: 0.9022 - val_loss: 0.8926\n",
            "Epoch 419/500\n",
            " - 2s - loss: 0.9022 - val_loss: 0.8926\n",
            "Epoch 420/500\n",
            " - 2s - loss: 0.9022 - val_loss: 0.8926\n",
            "Epoch 421/500\n",
            " - 2s - loss: 0.9022 - val_loss: 0.8926\n",
            "Epoch 422/500\n",
            " - 2s - loss: 0.9022 - val_loss: 0.8926\n",
            "Epoch 423/500\n",
            " - 2s - loss: 0.9022 - val_loss: 0.8926\n",
            "Epoch 424/500\n",
            " - 2s - loss: 0.9022 - val_loss: 0.8926\n",
            "Epoch 425/500\n",
            " - 2s - loss: 0.9022 - val_loss: 0.8926\n",
            "Epoch 426/500\n",
            " - 2s - loss: 0.9022 - val_loss: 0.8926\n",
            "Epoch 427/500\n",
            " - 2s - loss: 0.9022 - val_loss: 0.8926\n",
            "Epoch 428/500\n",
            " - 2s - loss: 0.9022 - val_loss: 0.8926\n",
            "Epoch 429/500\n",
            " - 2s - loss: 0.9022 - val_loss: 0.8926\n",
            "Epoch 430/500\n",
            " - 2s - loss: 0.9022 - val_loss: 0.8926\n",
            "Epoch 431/500\n",
            " - 2s - loss: 0.9022 - val_loss: 0.8926\n",
            "Epoch 432/500\n",
            " - 2s - loss: 0.9022 - val_loss: 0.8926\n",
            "Epoch 433/500\n",
            " - 2s - loss: 0.9022 - val_loss: 0.8926\n",
            "Epoch 434/500\n",
            " - 2s - loss: 0.9022 - val_loss: 0.8926\n",
            "Epoch 435/500\n",
            " - 2s - loss: 0.9022 - val_loss: 0.8926\n",
            "Epoch 436/500\n",
            " - 2s - loss: 0.9022 - val_loss: 0.8926\n",
            "Epoch 437/500\n",
            " - 2s - loss: 0.9022 - val_loss: 0.8926\n",
            "Epoch 438/500\n",
            " - 2s - loss: 0.9022 - val_loss: 0.8926\n",
            "Epoch 439/500\n",
            " - 2s - loss: 0.9022 - val_loss: 0.8926\n",
            "Epoch 440/500\n",
            " - 2s - loss: 0.9022 - val_loss: 0.8926\n",
            "Epoch 441/500\n",
            " - 2s - loss: 0.9022 - val_loss: 0.8926\n",
            "Epoch 442/500\n",
            " - 2s - loss: 0.9022 - val_loss: 0.8926\n",
            "Epoch 443/500\n",
            " - 2s - loss: 0.9022 - val_loss: 0.8926\n",
            "Epoch 444/500\n",
            " - 2s - loss: 0.9022 - val_loss: 0.8926\n",
            "Epoch 445/500\n",
            " - 2s - loss: 0.9022 - val_loss: 0.8926\n",
            "Epoch 446/500\n",
            " - 2s - loss: 0.9022 - val_loss: 0.8926\n",
            "Epoch 447/500\n",
            " - 2s - loss: 0.9022 - val_loss: 0.8926\n",
            "Epoch 448/500\n",
            " - 2s - loss: 0.9022 - val_loss: 0.8926\n",
            "Epoch 449/500\n",
            " - 2s - loss: 0.9022 - val_loss: 0.8926\n",
            "Epoch 450/500\n",
            " - 2s - loss: 0.9022 - val_loss: 0.8926\n",
            "Epoch 451/500\n",
            " - 2s - loss: 0.9022 - val_loss: 0.8926\n",
            "Epoch 452/500\n",
            " - 2s - loss: 0.9022 - val_loss: 0.8926\n",
            "Epoch 453/500\n",
            " - 2s - loss: 0.9022 - val_loss: 0.8926\n",
            "Epoch 454/500\n",
            " - 2s - loss: 0.9022 - val_loss: 0.8926\n",
            "Epoch 455/500\n",
            " - 2s - loss: 0.9022 - val_loss: 0.8926\n",
            "Epoch 456/500\n",
            " - 2s - loss: 0.9022 - val_loss: 0.8926\n",
            "Epoch 457/500\n",
            " - 2s - loss: 0.9022 - val_loss: 0.8926\n",
            "Epoch 458/500\n",
            " - 2s - loss: 0.9022 - val_loss: 0.8926\n",
            "Epoch 459/500\n",
            " - 2s - loss: 0.9022 - val_loss: 0.8926\n",
            "Epoch 460/500\n",
            " - 2s - loss: 0.9022 - val_loss: 0.8926\n",
            "Epoch 461/500\n",
            " - 2s - loss: 0.9022 - val_loss: 0.8926\n",
            "Epoch 462/500\n",
            " - 2s - loss: 0.9022 - val_loss: 0.8926\n",
            "Epoch 463/500\n",
            " - 2s - loss: 0.9022 - val_loss: 0.8926\n",
            "Epoch 464/500\n",
            " - 2s - loss: 0.9022 - val_loss: 0.8926\n",
            "Epoch 465/500\n",
            " - 2s - loss: 0.9022 - val_loss: 0.8926\n",
            "Epoch 466/500\n",
            " - 2s - loss: 0.9022 - val_loss: 0.8926\n",
            "Epoch 467/500\n",
            " - 2s - loss: 0.9022 - val_loss: 0.8926\n",
            "Epoch 468/500\n",
            " - 2s - loss: 0.9022 - val_loss: 0.8926\n",
            "Epoch 469/500\n",
            " - 2s - loss: 0.9022 - val_loss: 0.8926\n",
            "Epoch 470/500\n",
            " - 2s - loss: 0.9022 - val_loss: 0.8926\n",
            "Epoch 471/500\n",
            " - 2s - loss: 0.9022 - val_loss: 0.8926\n",
            "Epoch 472/500\n",
            " - 2s - loss: 0.9022 - val_loss: 0.8926\n",
            "Epoch 473/500\n",
            " - 2s - loss: 0.9022 - val_loss: 0.8926\n",
            "Epoch 474/500\n",
            " - 2s - loss: 0.9022 - val_loss: 0.8926\n",
            "Epoch 475/500\n",
            " - 2s - loss: 0.9022 - val_loss: 0.8926\n",
            "Epoch 476/500\n",
            " - 2s - loss: 0.9022 - val_loss: 0.8926\n",
            "Epoch 477/500\n",
            " - 2s - loss: 0.9022 - val_loss: 0.8926\n",
            "Epoch 478/500\n",
            " - 2s - loss: 0.9022 - val_loss: 0.8926\n",
            "Epoch 479/500\n",
            " - 2s - loss: 0.9022 - val_loss: 0.8926\n",
            "Epoch 480/500\n",
            " - 2s - loss: 0.9022 - val_loss: 0.8926\n",
            "Epoch 481/500\n",
            " - 2s - loss: 0.9022 - val_loss: 0.8926\n",
            "Epoch 482/500\n",
            " - 2s - loss: 0.9022 - val_loss: 0.8926\n",
            "Epoch 483/500\n",
            " - 2s - loss: 0.9022 - val_loss: 0.8926\n",
            "Epoch 484/500\n",
            " - 2s - loss: 0.9022 - val_loss: 0.8926\n",
            "Epoch 485/500\n",
            " - 2s - loss: 0.9022 - val_loss: 0.8926\n",
            "Epoch 486/500\n",
            " - 2s - loss: 0.9022 - val_loss: 0.8926\n",
            "Epoch 487/500\n",
            " - 2s - loss: 0.9022 - val_loss: 0.8926\n",
            "Epoch 488/500\n",
            " - 2s - loss: 0.9022 - val_loss: 0.8926\n",
            "Epoch 489/500\n",
            " - 2s - loss: 0.9022 - val_loss: 0.8926\n",
            "Epoch 490/500\n",
            " - 2s - loss: 0.9022 - val_loss: 0.8926\n",
            "Epoch 491/500\n",
            " - 2s - loss: 0.9022 - val_loss: 0.8926\n",
            "Epoch 492/500\n",
            " - 2s - loss: 0.9022 - val_loss: 0.8926\n",
            "Epoch 493/500\n",
            " - 2s - loss: 0.9022 - val_loss: 0.8926\n",
            "Epoch 494/500\n",
            " - 2s - loss: 0.9022 - val_loss: 0.8926\n",
            "Epoch 495/500\n",
            " - 2s - loss: 0.9022 - val_loss: 0.8926\n",
            "Epoch 496/500\n",
            " - 2s - loss: 0.9022 - val_loss: 0.8926\n",
            "Epoch 497/500\n",
            " - 2s - loss: 0.9022 - val_loss: 0.8926\n",
            "Epoch 498/500\n",
            " - 2s - loss: 0.9022 - val_loss: 0.8926\n",
            "Epoch 499/500\n",
            " - 2s - loss: 0.9022 - val_loss: 0.8926\n",
            "Epoch 500/500\n",
            " - 2s - loss: 0.9022 - val_loss: 0.8926\n",
            "CPU times: user 16min 57s, sys: 1min 14s, total: 18min 12s\n",
            "Wall time: 14min 38s\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4yGPbNfQNnNU",
        "colab_type": "code",
        "outputId": "92ad7eb4-f765-47dd-b8f4-865fd1e73ea3",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "#CRPS of train and test data \n",
        "(fc_model.evaluate(train_std_df_X, train_y, batch_size = 50, verbose=0), fc_model.evaluate(test_std_df_X, test_y, batch_size = 50, verbose=0))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(0.9003499075124675, 0.8935632188210798)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 58
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Bw96ehK32lO8",
        "colab_type": "text"
      },
      "source": [
        "## Feature importance "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1WZq0Vu8WDhj",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Feature importance\n",
        "\n",
        "\n",
        "#Feature importance for standardized scaled\n",
        "%matplotlib inline\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "# slice error add iloc\n",
        "def eval_shuf(m, idx, emb=False):\n",
        "    x_shuf = test_std_df_X.copy()\n",
        "    x_shuf.iloc[:, idx] = np.random.permutation(x_shuf.iloc[:, idx])\n",
        "    x = x_shuf\n",
        "    return m.evaluate(x, test_y, 4096, 0)\n",
        "def perm_imp(m):\n",
        "    scores = [eval_shuf(m, i) for i in range(len(test_X.columns))]\n",
        "    fimp = np.array(scores) - ref_score\n",
        "    df = pd.DataFrame(columns=['Feature', 'Importance'])\n",
        "    df['Feature'] = test_std_df_X.columns; df['Importance'] = fimp\n",
        "    return df\n",
        "def perm_imp(m):\n",
        "    scores = [eval_shuf(m, i) for i in range(len(test_X.columns))]\n",
        "    fimp = np.array(scores) - ref_score\n",
        "    df = pd.DataFrame(columns=['Feature', 'Importance'])\n",
        "    df['Feature'] = test_std_df_X.columns; df['Importance'] = fimp\n",
        "    return df\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cWblO6q2mYGN",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#fimp for fc\n",
        "ref_score = fc_model.evaluate(test_std_df_X, test_y, batch_size = 50, verbose=0)\n",
        "\n",
        "fimp_fc_standardized_model = perm_imp(fc_model)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mWrzZC5XWQjd",
        "colab_type": "code",
        "outputId": "b8c26073-3500-43aa-9b25-496b57b959d1",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 404
        }
      },
      "source": [
        "#fimp for fc\n",
        "ref_score = fc_model.evaluate(test_std_df_X, test_y, batch_size = 50, verbose=0)\n",
        "\n",
        "fimp_fc_standardized_model = perm_imp(fc_model)\n",
        "fig, ax = plt.subplots(figsize=(12, 5))\n",
        "sns.barplot(data=fimp_fc_standardized_model, y='Importance', x='Feature', ax=ax)\n",
        "plt.xticks(rotation=90);\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAscAAAGDCAYAAADH173JAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n/wLyZrIkGZfLPgAAAABJRU5ErkJggg==\n",
            "text/plain": [
              "<Figure size 864x360 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9wMf3oaNDFWp",
        "colab_type": "text"
      },
      "source": [
        "## Neural network (NN) model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NQAGRn87VZXC",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Neural network, one hidden layer\n",
        "\n",
        "def build_hidden_model(n_features, n_outputs, hidden_nodes, compile=False,\n",
        "                       optimizer='adam', lr=0.01, loss=crps_cost_function,\n",
        "                       activation='relu'):\n",
        "    \"\"\"Build (and compile) a neural net with hidden layers\n",
        "    Args:\n",
        "        n_features: Number of features\n",
        "        n_outputs: Number of outputs\n",
        "        hidden_nodes: int or list of hidden nodes\n",
        "        compile: If true, compile model\n",
        "        optimizer: Name of optimizer\n",
        "        lr: learning rate\n",
        "        loss: loss function\n",
        "    Returns:\n",
        "        model: Keras model\n",
        "    \"\"\"\n",
        "    if type(hidden_nodes) is not list:\n",
        "        hidden_nodes = [hidden_nodes]\n",
        "    inp = Input(shape=(n_features,))\n",
        "    x = Dense(hidden_nodes[0], activation=activation)(inp)\n",
        "    if len(hidden_nodes) > 1:\n",
        "        for h in hidden_nodes[1:]:\n",
        "            x = Dense(h, activation=activation)(x)\n",
        "    x = Dense(n_outputs, activation='linear')(x)\n",
        "    model = Model(inputs=inp, outputs=x)\n",
        "\n",
        "    if compile:\n",
        "        opt = keras.optimizers.__dict__[optimizer](lr=lr)\n",
        "        model.compile(optimizer=opt, loss=loss)\n",
        "    return model\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qMEY3OlqVZPZ",
        "colab_type": "code",
        "outputId": "5c63f722-7d49-4e06-a01d-820f015f975a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 278
        }
      },
      "source": [
        "input_features =  len(train_standardized_X[0])\n",
        "print(input_features)\n",
        "hidden_model = build_hidden_model(input_features, 2, hidden_nodes=[50], compile=True)\n",
        "hidden_model.summary()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "26\n",
            "Model: \"model_4\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "input_4 (InputLayer)         (None, 26)                0         \n",
            "_________________________________________________________________\n",
            "dense_5 (Dense)              (None, 50)                1350      \n",
            "_________________________________________________________________\n",
            "dense_6 (Dense)              (None, 2)                 102       \n",
            "=================================================================\n",
            "Total params: 1,452\n",
            "Trainable params: 1,452\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lFSnSqLFVZFx",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n",
        "hidden_model.compile( keras.optimizers.Adam(0.001), loss=crps_cost_function)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RsxvX5i41hcL",
        "colab_type": "code",
        "outputId": "d1024b3b-4b38-4736-de34-be453e5d8b68",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "#Hidden model with distance to coast second run\n",
        "%%time\n",
        "hidden_model.fit(train_standardized_X, train_y, epochs=500, batch_size = 50,\n",
        "                 validation_split=0.3, verbose=2, shuffle=False)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Train on 62597 samples, validate on 26828 samples\n",
            "Epoch 1/500\n",
            " - 2s - loss: 1.9083 - val_loss: 1.0060\n",
            "Epoch 2/500\n",
            " - 2s - loss: 0.9447 - val_loss: 0.9095\n",
            "Epoch 3/500\n",
            " - 2s - loss: 0.8840 - val_loss: 0.8729\n",
            "Epoch 4/500\n",
            " - 2s - loss: 0.8584 - val_loss: 0.8568\n",
            "Epoch 5/500\n",
            " - 2s - loss: 0.8459 - val_loss: 0.8481\n",
            "Epoch 6/500\n",
            " - 2s - loss: 0.8383 - val_loss: 0.8428\n",
            "Epoch 7/500\n",
            " - 2s - loss: 0.8331 - val_loss: 0.8387\n",
            "Epoch 8/500\n",
            " - 2s - loss: 0.8291 - val_loss: 0.8356\n",
            "Epoch 9/500\n",
            " - 2s - loss: 0.8259 - val_loss: 0.8335\n",
            "Epoch 10/500\n",
            " - 2s - loss: 0.8232 - val_loss: 0.8313\n",
            "Epoch 11/500\n",
            " - 2s - loss: 0.8208 - val_loss: 0.8294\n",
            "Epoch 12/500\n",
            " - 2s - loss: 0.8186 - val_loss: 0.8277\n",
            "Epoch 13/500\n",
            " - 2s - loss: 0.8166 - val_loss: 0.8261\n",
            "Epoch 14/500\n",
            " - 2s - loss: 0.8150 - val_loss: 0.8249\n",
            "Epoch 15/500\n",
            " - 2s - loss: 0.8136 - val_loss: 0.8239\n",
            "Epoch 16/500\n",
            " - 2s - loss: 0.8124 - val_loss: 0.8230\n",
            "Epoch 17/500\n",
            " - 2s - loss: 0.8113 - val_loss: 0.8223\n",
            "Epoch 18/500\n",
            " - 2s - loss: 0.8104 - val_loss: 0.8216\n",
            "Epoch 19/500\n",
            " - 2s - loss: 0.8095 - val_loss: 0.8211\n",
            "Epoch 20/500\n",
            " - 2s - loss: 0.8088 - val_loss: 0.8206\n",
            "Epoch 21/500\n",
            " - 2s - loss: 0.8081 - val_loss: 0.8202\n",
            "Epoch 22/500\n",
            " - 2s - loss: 0.8076 - val_loss: 0.8197\n",
            "Epoch 23/500\n",
            " - 2s - loss: 0.8070 - val_loss: 0.8192\n",
            "Epoch 24/500\n",
            " - 2s - loss: 0.8065 - val_loss: 0.8188\n",
            "Epoch 25/500\n",
            " - 2s - loss: 0.8060 - val_loss: 0.8184\n",
            "Epoch 26/500\n",
            " - 2s - loss: 0.8055 - val_loss: 0.8181\n",
            "Epoch 27/500\n",
            " - 2s - loss: 0.8051 - val_loss: 0.8175\n",
            "Epoch 28/500\n",
            " - 2s - loss: 0.8047 - val_loss: 0.8172\n",
            "Epoch 29/500\n",
            " - 2s - loss: 0.8042 - val_loss: 0.8168\n",
            "Epoch 30/500\n",
            " - 2s - loss: 0.8038 - val_loss: 0.8164\n",
            "Epoch 31/500\n",
            " - 2s - loss: 0.8034 - val_loss: 0.8159\n",
            "Epoch 32/500\n",
            " - 2s - loss: 0.8030 - val_loss: 0.8158\n",
            "Epoch 33/500\n",
            " - 2s - loss: 0.8027 - val_loss: 0.8155\n",
            "Epoch 34/500\n",
            " - 2s - loss: 0.8024 - val_loss: 0.8152\n",
            "Epoch 35/500\n",
            " - 2s - loss: 0.8021 - val_loss: 0.8151\n",
            "Epoch 36/500\n",
            " - 2s - loss: 0.8018 - val_loss: 0.8149\n",
            "Epoch 37/500\n",
            " - 2s - loss: 0.8015 - val_loss: 0.8147\n",
            "Epoch 38/500\n",
            " - 2s - loss: 0.8012 - val_loss: 0.8146\n",
            "Epoch 39/500\n",
            " - 2s - loss: 0.8010 - val_loss: 0.8145\n",
            "Epoch 40/500\n",
            " - 2s - loss: 0.8008 - val_loss: 0.8142\n",
            "Epoch 41/500\n",
            " - 2s - loss: 0.8005 - val_loss: 0.8141\n",
            "Epoch 42/500\n",
            " - 2s - loss: 0.8003 - val_loss: 0.8138\n",
            "Epoch 43/500\n",
            " - 2s - loss: 0.8000 - val_loss: 0.8137\n",
            "Epoch 44/500\n",
            " - 2s - loss: 0.7998 - val_loss: 0.8134\n",
            "Epoch 45/500\n",
            " - 2s - loss: 0.7996 - val_loss: 0.8133\n",
            "Epoch 46/500\n",
            " - 2s - loss: 0.7994 - val_loss: 0.8130\n",
            "Epoch 47/500\n",
            " - 2s - loss: 0.7992 - val_loss: 0.8129\n",
            "Epoch 48/500\n",
            " - 2s - loss: 0.7990 - val_loss: 0.8129\n",
            "Epoch 49/500\n",
            " - 2s - loss: 0.7988 - val_loss: 0.8126\n",
            "Epoch 50/500\n",
            " - 2s - loss: 0.7986 - val_loss: 0.8123\n",
            "Epoch 51/500\n",
            " - 2s - loss: 0.7984 - val_loss: 0.8122\n",
            "Epoch 52/500\n",
            " - 2s - loss: 0.7982 - val_loss: 0.8119\n",
            "Epoch 53/500\n",
            " - 2s - loss: 0.7981 - val_loss: 0.8119\n",
            "Epoch 54/500\n",
            " - 2s - loss: 0.7979 - val_loss: 0.8118\n",
            "Epoch 55/500\n",
            " - 2s - loss: 0.7978 - val_loss: 0.8116\n",
            "Epoch 56/500\n",
            " - 2s - loss: 0.7976 - val_loss: 0.8115\n",
            "Epoch 57/500\n",
            " - 2s - loss: 0.7975 - val_loss: 0.8114\n",
            "Epoch 58/500\n",
            " - 2s - loss: 0.7973 - val_loss: 0.8113\n",
            "Epoch 59/500\n",
            " - 2s - loss: 0.7972 - val_loss: 0.8112\n",
            "Epoch 60/500\n",
            " - 2s - loss: 0.7969 - val_loss: 0.8110\n",
            "Epoch 61/500\n",
            " - 2s - loss: 0.7968 - val_loss: 0.8110\n",
            "Epoch 62/500\n",
            " - 2s - loss: 0.7966 - val_loss: 0.8107\n",
            "Epoch 63/500\n",
            " - 2s - loss: 0.7964 - val_loss: 0.8104\n",
            "Epoch 64/500\n",
            " - 2s - loss: 0.7962 - val_loss: 0.8102\n",
            "Epoch 65/500\n",
            " - 2s - loss: 0.7960 - val_loss: 0.8099\n",
            "Epoch 66/500\n",
            " - 2s - loss: 0.7959 - val_loss: 0.8096\n",
            "Epoch 67/500\n",
            " - 2s - loss: 0.7957 - val_loss: 0.8094\n",
            "Epoch 68/500\n",
            " - 2s - loss: 0.7956 - val_loss: 0.8092\n",
            "Epoch 69/500\n",
            " - 2s - loss: 0.7954 - val_loss: 0.8090\n",
            "Epoch 70/500\n",
            " - 2s - loss: 0.7953 - val_loss: 0.8087\n",
            "Epoch 71/500\n",
            " - 2s - loss: 0.7951 - val_loss: 0.8084\n",
            "Epoch 72/500\n",
            " - 2s - loss: 0.7950 - val_loss: 0.8082\n",
            "Epoch 73/500\n",
            " - 2s - loss: 0.7948 - val_loss: 0.8080\n",
            "Epoch 74/500\n",
            " - 2s - loss: 0.7946 - val_loss: 0.8080\n",
            "Epoch 75/500\n",
            " - 2s - loss: 0.7945 - val_loss: 0.8077\n",
            "Epoch 76/500\n",
            " - 2s - loss: 0.7943 - val_loss: 0.8076\n",
            "Epoch 77/500\n",
            " - 2s - loss: 0.7942 - val_loss: 0.8073\n",
            "Epoch 78/500\n",
            " - 2s - loss: 0.7941 - val_loss: 0.8072\n",
            "Epoch 79/500\n",
            " - 2s - loss: 0.7939 - val_loss: 0.8071\n",
            "Epoch 80/500\n",
            " - 2s - loss: 0.7939 - val_loss: 0.8071\n",
            "Epoch 81/500\n",
            " - 2s - loss: 0.7937 - val_loss: 0.8070\n",
            "Epoch 82/500\n",
            " - 2s - loss: 0.7937 - val_loss: 0.8069\n",
            "Epoch 83/500\n",
            " - 2s - loss: 0.7935 - val_loss: 0.8068\n",
            "Epoch 84/500\n",
            " - 2s - loss: 0.7935 - val_loss: 0.8067\n",
            "Epoch 85/500\n",
            " - 2s - loss: 0.7934 - val_loss: 0.8067\n",
            "Epoch 86/500\n",
            " - 2s - loss: 0.7933 - val_loss: 0.8065\n",
            "Epoch 87/500\n",
            " - 2s - loss: 0.7932 - val_loss: 0.8065\n",
            "Epoch 88/500\n",
            " - 2s - loss: 0.7931 - val_loss: 0.8064\n",
            "Epoch 89/500\n",
            " - 2s - loss: 0.7930 - val_loss: 0.8063\n",
            "Epoch 90/500\n",
            " - 2s - loss: 0.7930 - val_loss: 0.8062\n",
            "Epoch 91/500\n",
            " - 2s - loss: 0.7929 - val_loss: 0.8061\n",
            "Epoch 92/500\n",
            " - 2s - loss: 0.7928 - val_loss: 0.8061\n",
            "Epoch 93/500\n",
            " - 2s - loss: 0.7927 - val_loss: 0.8060\n",
            "Epoch 94/500\n",
            " - 2s - loss: 0.7926 - val_loss: 0.8060\n",
            "Epoch 95/500\n",
            " - 2s - loss: 0.7925 - val_loss: 0.8059\n",
            "Epoch 96/500\n",
            " - 2s - loss: 0.7924 - val_loss: 0.8059\n",
            "Epoch 97/500\n",
            " - 2s - loss: 0.7924 - val_loss: 0.8059\n",
            "Epoch 98/500\n",
            " - 2s - loss: 0.7923 - val_loss: 0.8058\n",
            "Epoch 99/500\n",
            " - 2s - loss: 0.7922 - val_loss: 0.8058\n",
            "Epoch 100/500\n",
            " - 2s - loss: 0.7921 - val_loss: 0.8057\n",
            "Epoch 101/500\n",
            " - 2s - loss: 0.7920 - val_loss: 0.8057\n",
            "Epoch 102/500\n",
            " - 2s - loss: 0.7920 - val_loss: 0.8057\n",
            "Epoch 103/500\n",
            " - 2s - loss: 0.7919 - val_loss: 0.8056\n",
            "Epoch 104/500\n",
            " - 2s - loss: 0.7919 - val_loss: 0.8057\n",
            "Epoch 105/500\n",
            " - 2s - loss: 0.7918 - val_loss: 0.8056\n",
            "Epoch 106/500\n",
            " - 2s - loss: 0.7917 - val_loss: 0.8055\n",
            "Epoch 107/500\n",
            " - 2s - loss: 0.7917 - val_loss: 0.8055\n",
            "Epoch 108/500\n",
            " - 2s - loss: 0.7916 - val_loss: 0.8055\n",
            "Epoch 109/500\n",
            " - 2s - loss: 0.7916 - val_loss: 0.8054\n",
            "Epoch 110/500\n",
            " - 2s - loss: 0.7915 - val_loss: 0.8054\n",
            "Epoch 111/500\n",
            " - 2s - loss: 0.7915 - val_loss: 0.8054\n",
            "Epoch 112/500\n",
            " - 2s - loss: 0.7914 - val_loss: 0.8053\n",
            "Epoch 113/500\n",
            " - 2s - loss: 0.7913 - val_loss: 0.8053\n",
            "Epoch 114/500\n",
            " - 2s - loss: 0.7913 - val_loss: 0.8053\n",
            "Epoch 115/500\n",
            " - 2s - loss: 0.7913 - val_loss: 0.8052\n",
            "Epoch 116/500\n",
            " - 2s - loss: 0.7912 - val_loss: 0.8052\n",
            "Epoch 117/500\n",
            " - 2s - loss: 0.7912 - val_loss: 0.8051\n",
            "Epoch 118/500\n",
            " - 2s - loss: 0.7910 - val_loss: 0.8050\n",
            "Epoch 119/500\n",
            " - 2s - loss: 0.7911 - val_loss: 0.8050\n",
            "Epoch 120/500\n",
            " - 2s - loss: 0.7909 - val_loss: 0.8050\n",
            "Epoch 121/500\n",
            " - 2s - loss: 0.7909 - val_loss: 0.8049\n",
            "Epoch 122/500\n",
            " - 2s - loss: 0.7909 - val_loss: 0.8048\n",
            "Epoch 123/500\n",
            " - 2s - loss: 0.7908 - val_loss: 0.8048\n",
            "Epoch 124/500\n",
            " - 2s - loss: 0.7908 - val_loss: 0.8047\n",
            "Epoch 125/500\n",
            " - 2s - loss: 0.7907 - val_loss: 0.8047\n",
            "Epoch 126/500\n",
            " - 2s - loss: 0.7908 - val_loss: 0.8047\n",
            "Epoch 127/500\n",
            " - 2s - loss: 0.7905 - val_loss: 0.8048\n",
            "Epoch 128/500\n",
            " - 2s - loss: 0.7906 - val_loss: 0.8047\n",
            "Epoch 129/500\n",
            " - 2s - loss: 0.7907 - val_loss: 0.8047\n",
            "Epoch 130/500\n",
            " - 2s - loss: 0.7905 - val_loss: 0.8047\n",
            "Epoch 131/500\n",
            " - 2s - loss: 0.7904 - val_loss: 0.8046\n",
            "Epoch 132/500\n",
            " - 2s - loss: 0.7905 - val_loss: 0.8046\n",
            "Epoch 133/500\n",
            " - 2s - loss: 0.7903 - val_loss: 0.8047\n",
            "Epoch 134/500\n",
            " - 2s - loss: 0.7903 - val_loss: 0.8046\n",
            "Epoch 135/500\n",
            " - 2s - loss: 0.7903 - val_loss: 0.8046\n",
            "Epoch 136/500\n",
            " - 2s - loss: 0.7902 - val_loss: 0.8045\n",
            "Epoch 137/500\n",
            " - 2s - loss: 0.7903 - val_loss: 0.8045\n",
            "Epoch 138/500\n",
            " - 2s - loss: 0.7901 - val_loss: 0.8046\n",
            "Epoch 139/500\n",
            " - 2s - loss: 0.7900 - val_loss: 0.8044\n",
            "Epoch 140/500\n",
            " - 2s - loss: 0.7901 - val_loss: 0.8045\n",
            "Epoch 141/500\n",
            " - 2s - loss: 0.7899 - val_loss: 0.8045\n",
            "Epoch 142/500\n",
            " - 2s - loss: 0.7899 - val_loss: 0.8044\n",
            "Epoch 143/500\n",
            " - 2s - loss: 0.7900 - val_loss: 0.8044\n",
            "Epoch 144/500\n",
            " - 2s - loss: 0.7898 - val_loss: 0.8043\n",
            "Epoch 145/500\n",
            " - 2s - loss: 0.7900 - val_loss: 0.8044\n",
            "Epoch 146/500\n",
            " - 2s - loss: 0.7898 - val_loss: 0.8044\n",
            "Epoch 147/500\n",
            " - 2s - loss: 0.7897 - val_loss: 0.8043\n",
            "Epoch 148/500\n",
            " - 2s - loss: 0.7899 - val_loss: 0.8043\n",
            "Epoch 149/500\n",
            " - 2s - loss: 0.7897 - val_loss: 0.8044\n",
            "Epoch 150/500\n",
            " - 2s - loss: 0.7896 - val_loss: 0.8042\n",
            "Epoch 151/500\n",
            " - 2s - loss: 0.7897 - val_loss: 0.8043\n",
            "Epoch 152/500\n",
            " - 2s - loss: 0.7896 - val_loss: 0.8043\n",
            "Epoch 153/500\n",
            " - 2s - loss: 0.7894 - val_loss: 0.8042\n",
            "Epoch 154/500\n",
            " - 2s - loss: 0.7897 - val_loss: 0.8042\n",
            "Epoch 155/500\n",
            " - 2s - loss: 0.7895 - val_loss: 0.8042\n",
            "Epoch 156/500\n",
            " - 2s - loss: 0.7894 - val_loss: 0.8041\n",
            "Epoch 157/500\n",
            " - 2s - loss: 0.7896 - val_loss: 0.8042\n",
            "Epoch 158/500\n",
            " - 2s - loss: 0.7896 - val_loss: 0.8041\n",
            "Epoch 159/500\n",
            " - 2s - loss: 0.7893 - val_loss: 0.8042\n",
            "Epoch 160/500\n",
            " - 2s - loss: 0.7895 - val_loss: 0.8042\n",
            "Epoch 161/500\n",
            " - 2s - loss: 0.7894 - val_loss: 0.8042\n",
            "Epoch 162/500\n",
            " - 2s - loss: 0.7892 - val_loss: 0.8042\n",
            "Epoch 163/500\n",
            " - 2s - loss: 0.7895 - val_loss: 0.8043\n",
            "Epoch 164/500\n",
            " - 2s - loss: 0.7894 - val_loss: 0.8042\n",
            "Epoch 165/500\n",
            " - 2s - loss: 0.7891 - val_loss: 0.8042\n",
            "Epoch 166/500\n",
            " - 2s - loss: 0.7894 - val_loss: 0.8043\n",
            "Epoch 167/500\n",
            " - 2s - loss: 0.7892 - val_loss: 0.8042\n",
            "Epoch 168/500\n",
            " - 2s - loss: 0.7891 - val_loss: 0.8041\n",
            "Epoch 169/500\n",
            " - 2s - loss: 0.7893 - val_loss: 0.8041\n",
            "Epoch 170/500\n",
            " - 2s - loss: 0.7891 - val_loss: 0.8040\n",
            "Epoch 171/500\n",
            " - 2s - loss: 0.7890 - val_loss: 0.8040\n",
            "Epoch 172/500\n",
            " - 2s - loss: 0.7892 - val_loss: 0.8040\n",
            "Epoch 173/500\n",
            " - 2s - loss: 0.7890 - val_loss: 0.8040\n",
            "Epoch 174/500\n",
            " - 2s - loss: 0.7889 - val_loss: 0.8039\n",
            "Epoch 175/500\n",
            " - 2s - loss: 0.7890 - val_loss: 0.8040\n",
            "Epoch 176/500\n",
            " - 2s - loss: 0.7888 - val_loss: 0.8040\n",
            "Epoch 177/500\n",
            " - 2s - loss: 0.7888 - val_loss: 0.8039\n",
            "Epoch 178/500\n",
            " - 2s - loss: 0.7889 - val_loss: 0.8040\n",
            "Epoch 179/500\n",
            " - 2s - loss: 0.7888 - val_loss: 0.8039\n",
            "Epoch 180/500\n",
            " - 2s - loss: 0.7887 - val_loss: 0.8039\n",
            "Epoch 181/500\n",
            " - 2s - loss: 0.7888 - val_loss: 0.8039\n",
            "Epoch 182/500\n",
            " - 2s - loss: 0.7886 - val_loss: 0.8039\n",
            "Epoch 183/500\n",
            " - 2s - loss: 0.7886 - val_loss: 0.8038\n",
            "Epoch 184/500\n",
            " - 2s - loss: 0.7888 - val_loss: 0.8039\n",
            "Epoch 185/500\n",
            " - 2s - loss: 0.7886 - val_loss: 0.8039\n",
            "Epoch 186/500\n",
            " - 2s - loss: 0.7885 - val_loss: 0.8039\n",
            "Epoch 187/500\n",
            " - 2s - loss: 0.7886 - val_loss: 0.8039\n",
            "Epoch 188/500\n",
            " - 2s - loss: 0.7884 - val_loss: 0.8039\n",
            "Epoch 189/500\n",
            " - 2s - loss: 0.7884 - val_loss: 0.8038\n",
            "Epoch 190/500\n",
            " - 2s - loss: 0.7885 - val_loss: 0.8038\n",
            "Epoch 191/500\n",
            " - 2s - loss: 0.7884 - val_loss: 0.8039\n",
            "Epoch 192/500\n",
            " - 2s - loss: 0.7883 - val_loss: 0.8038\n",
            "Epoch 193/500\n",
            " - 2s - loss: 0.7885 - val_loss: 0.8038\n",
            "Epoch 194/500\n",
            " - 2s - loss: 0.7883 - val_loss: 0.8038\n",
            "Epoch 195/500\n",
            " - 2s - loss: 0.7882 - val_loss: 0.8038\n",
            "Epoch 196/500\n",
            " - 2s - loss: 0.7884 - val_loss: 0.8039\n",
            "Epoch 197/500\n",
            " - 2s - loss: 0.7883 - val_loss: 0.8038\n",
            "Epoch 198/500\n",
            " - 2s - loss: 0.7882 - val_loss: 0.8038\n",
            "Epoch 199/500\n",
            " - 2s - loss: 0.7883 - val_loss: 0.8038\n",
            "Epoch 200/500\n",
            " - 2s - loss: 0.7881 - val_loss: 0.8039\n",
            "Epoch 201/500\n",
            " - 2s - loss: 0.7881 - val_loss: 0.8038\n",
            "Epoch 202/500\n",
            " - 2s - loss: 0.7882 - val_loss: 0.8038\n",
            "Epoch 203/500\n",
            " - 2s - loss: 0.7881 - val_loss: 0.8038\n",
            "Epoch 204/500\n",
            " - 2s - loss: 0.7880 - val_loss: 0.8037\n",
            "Epoch 205/500\n",
            " - 2s - loss: 0.7880 - val_loss: 0.8038\n",
            "Epoch 206/500\n",
            " - 2s - loss: 0.7880 - val_loss: 0.8038\n",
            "Epoch 207/500\n",
            " - 2s - loss: 0.7880 - val_loss: 0.8039\n",
            "Epoch 208/500\n",
            " - 2s - loss: 0.7879 - val_loss: 0.8039\n",
            "Epoch 209/500\n",
            " - 2s - loss: 0.7879 - val_loss: 0.8039\n",
            "Epoch 210/500\n",
            " - 2s - loss: 0.7879 - val_loss: 0.8038\n",
            "Epoch 211/500\n",
            " - 2s - loss: 0.7879 - val_loss: 0.8037\n",
            "Epoch 212/500\n",
            " - 2s - loss: 0.7880 - val_loss: 0.8037\n",
            "Epoch 213/500\n",
            " - 2s - loss: 0.7878 - val_loss: 0.8036\n",
            "Epoch 214/500\n",
            " - 2s - loss: 0.7881 - val_loss: 0.8036\n",
            "Epoch 215/500\n",
            " - 2s - loss: 0.7879 - val_loss: 0.8035\n",
            "Epoch 216/500\n",
            " - 2s - loss: 0.7877 - val_loss: 0.8036\n",
            "Epoch 217/500\n",
            " - 2s - loss: 0.7879 - val_loss: 0.8037\n",
            "Epoch 218/500\n",
            " - 2s - loss: 0.7878 - val_loss: 0.8037\n",
            "Epoch 219/500\n",
            " - 2s - loss: 0.7877 - val_loss: 0.8036\n",
            "Epoch 220/500\n",
            " - 2s - loss: 0.7878 - val_loss: 0.8036\n",
            "Epoch 221/500\n",
            " - 2s - loss: 0.7877 - val_loss: 0.8036\n",
            "Epoch 222/500\n",
            " - 2s - loss: 0.7877 - val_loss: 0.8036\n",
            "Epoch 223/500\n",
            " - 2s - loss: 0.7876 - val_loss: 0.8036\n",
            "Epoch 224/500\n",
            " - 2s - loss: 0.7877 - val_loss: 0.8036\n",
            "Epoch 225/500\n",
            " - 2s - loss: 0.7876 - val_loss: 0.8036\n",
            "Epoch 226/500\n",
            " - 2s - loss: 0.7876 - val_loss: 0.8036\n",
            "Epoch 227/500\n",
            " - 2s - loss: 0.7875 - val_loss: 0.8036\n",
            "Epoch 228/500\n",
            " - 2s - loss: 0.7876 - val_loss: 0.8035\n",
            "Epoch 229/500\n",
            " - 2s - loss: 0.7874 - val_loss: 0.8036\n",
            "Epoch 230/500\n",
            " - 2s - loss: 0.7880 - val_loss: 0.8034\n",
            "Epoch 231/500\n",
            " - 2s - loss: 0.7874 - val_loss: 0.8035\n",
            "Epoch 232/500\n",
            " - 2s - loss: 0.7874 - val_loss: 0.8035\n",
            "Epoch 233/500\n",
            " - 2s - loss: 0.7873 - val_loss: 0.8035\n",
            "Epoch 234/500\n",
            " - 2s - loss: 0.7874 - val_loss: 0.8035\n",
            "Epoch 235/500\n",
            " - 2s - loss: 0.7873 - val_loss: 0.8035\n",
            "Epoch 236/500\n",
            " - 2s - loss: 0.7873 - val_loss: 0.8034\n",
            "Epoch 237/500\n",
            " - 3s - loss: 0.7873 - val_loss: 0.8035\n",
            "Epoch 238/500\n",
            " - 2s - loss: 0.7873 - val_loss: 0.8034\n",
            "Epoch 239/500\n",
            " - 2s - loss: 0.7872 - val_loss: 0.8034\n",
            "Epoch 240/500\n",
            " - 2s - loss: 0.7872 - val_loss: 0.8033\n",
            "Epoch 241/500\n",
            " - 2s - loss: 0.7872 - val_loss: 0.8034\n",
            "Epoch 242/500\n",
            " - 2s - loss: 0.7872 - val_loss: 0.8034\n",
            "Epoch 243/500\n",
            " - 2s - loss: 0.7871 - val_loss: 0.8034\n",
            "Epoch 244/500\n",
            " - 2s - loss: 0.7871 - val_loss: 0.8033\n",
            "Epoch 245/500\n",
            " - 2s - loss: 0.7870 - val_loss: 0.8033\n",
            "Epoch 246/500\n",
            " - 2s - loss: 0.7872 - val_loss: 0.8032\n",
            "Epoch 247/500\n",
            " - 2s - loss: 0.7871 - val_loss: 0.8031\n",
            "Epoch 248/500\n",
            " - 2s - loss: 0.7872 - val_loss: 0.8033\n",
            "Epoch 249/500\n",
            " - 2s - loss: 0.7872 - val_loss: 0.8032\n",
            "Epoch 250/500\n",
            " - 2s - loss: 0.7869 - val_loss: 0.8032\n",
            "Epoch 251/500\n",
            " - 2s - loss: 0.7871 - val_loss: 0.8032\n",
            "Epoch 252/500\n",
            " - 2s - loss: 0.7870 - val_loss: 0.8031\n",
            "Epoch 253/500\n",
            " - 2s - loss: 0.7868 - val_loss: 0.8030\n",
            "Epoch 254/500\n",
            " - 2s - loss: 0.7870 - val_loss: 0.8031\n",
            "Epoch 255/500\n",
            " - 2s - loss: 0.7869 - val_loss: 0.8030\n",
            "Epoch 256/500\n",
            " - 2s - loss: 0.7867 - val_loss: 0.8029\n",
            "Epoch 257/500\n",
            " - 2s - loss: 0.7870 - val_loss: 0.8030\n",
            "Epoch 258/500\n",
            " - 2s - loss: 0.7868 - val_loss: 0.8029\n",
            "Epoch 259/500\n",
            " - 2s - loss: 0.7866 - val_loss: 0.8029\n",
            "Epoch 260/500\n",
            " - 2s - loss: 0.7869 - val_loss: 0.8029\n",
            "Epoch 261/500\n",
            " - 2s - loss: 0.7867 - val_loss: 0.8028\n",
            "Epoch 262/500\n",
            " - 2s - loss: 0.7865 - val_loss: 0.8028\n",
            "Epoch 263/500\n",
            " - 2s - loss: 0.7868 - val_loss: 0.8028\n",
            "Epoch 264/500\n",
            " - 2s - loss: 0.7866 - val_loss: 0.8028\n",
            "Epoch 265/500\n",
            " - 2s - loss: 0.7864 - val_loss: 0.8027\n",
            "Epoch 266/500\n",
            " - 2s - loss: 0.7867 - val_loss: 0.8028\n",
            "Epoch 267/500\n",
            " - 2s - loss: 0.7865 - val_loss: 0.8027\n",
            "Epoch 268/500\n",
            " - 2s - loss: 0.7864 - val_loss: 0.8026\n",
            "Epoch 269/500\n",
            " - 2s - loss: 0.7867 - val_loss: 0.8028\n",
            "Epoch 270/500\n",
            " - 2s - loss: 0.7865 - val_loss: 0.8027\n",
            "Epoch 271/500\n",
            " - 2s - loss: 0.7863 - val_loss: 0.8026\n",
            "Epoch 272/500\n",
            " - 2s - loss: 0.7866 - val_loss: 0.8027\n",
            "Epoch 273/500\n",
            " - 2s - loss: 0.7864 - val_loss: 0.8027\n",
            "Epoch 274/500\n",
            " - 2s - loss: 0.7863 - val_loss: 0.8026\n",
            "Epoch 275/500\n",
            " - 2s - loss: 0.7865 - val_loss: 0.8027\n",
            "Epoch 276/500\n",
            " - 2s - loss: 0.7863 - val_loss: 0.8026\n",
            "Epoch 277/500\n",
            " - 2s - loss: 0.7862 - val_loss: 0.8025\n",
            "Epoch 278/500\n",
            " - 2s - loss: 0.7864 - val_loss: 0.8026\n",
            "Epoch 279/500\n",
            " - 2s - loss: 0.7863 - val_loss: 0.8025\n",
            "Epoch 280/500\n",
            " - 2s - loss: 0.7861 - val_loss: 0.8025\n",
            "Epoch 281/500\n",
            " - 2s - loss: 0.7864 - val_loss: 0.8026\n",
            "Epoch 282/500\n",
            " - 2s - loss: 0.7862 - val_loss: 0.8024\n",
            "Epoch 283/500\n",
            " - 2s - loss: 0.7860 - val_loss: 0.8024\n",
            "Epoch 284/500\n",
            " - 2s - loss: 0.7863 - val_loss: 0.8025\n",
            "Epoch 285/500\n",
            " - 2s - loss: 0.7861 - val_loss: 0.8024\n",
            "Epoch 286/500\n",
            " - 2s - loss: 0.7860 - val_loss: 0.8024\n",
            "Epoch 287/500\n",
            " - 2s - loss: 0.7862 - val_loss: 0.8024\n",
            "Epoch 288/500\n",
            " - 2s - loss: 0.7860 - val_loss: 0.8024\n",
            "Epoch 289/500\n",
            " - 2s - loss: 0.7859 - val_loss: 0.8024\n",
            "Epoch 290/500\n",
            " - 2s - loss: 0.7861 - val_loss: 0.8024\n",
            "Epoch 291/500\n",
            " - 2s - loss: 0.7859 - val_loss: 0.8023\n",
            "Epoch 292/500\n",
            " - 2s - loss: 0.7858 - val_loss: 0.8023\n",
            "Epoch 293/500\n",
            " - 2s - loss: 0.7861 - val_loss: 0.8024\n",
            "Epoch 294/500\n",
            " - 2s - loss: 0.7859 - val_loss: 0.8023\n",
            "Epoch 295/500\n",
            " - 2s - loss: 0.7857 - val_loss: 0.8023\n",
            "Epoch 296/500\n",
            " - 2s - loss: 0.7860 - val_loss: 0.8022\n",
            "Epoch 297/500\n",
            " - 2s - loss: 0.7859 - val_loss: 0.8021\n",
            "Epoch 298/500\n",
            " - 2s - loss: 0.7857 - val_loss: 0.8020\n",
            "Epoch 299/500\n",
            " - 2s - loss: 0.7859 - val_loss: 0.8022\n",
            "Epoch 300/500\n",
            " - 2s - loss: 0.7858 - val_loss: 0.8021\n",
            "Epoch 301/500\n",
            " - 2s - loss: 0.7856 - val_loss: 0.8021\n",
            "Epoch 302/500\n",
            " - 2s - loss: 0.7859 - val_loss: 0.8022\n",
            "Epoch 303/500\n",
            " - 2s - loss: 0.7858 - val_loss: 0.8021\n",
            "Epoch 304/500\n",
            " - 2s - loss: 0.7855 - val_loss: 0.8021\n",
            "Epoch 305/500\n",
            " - 2s - loss: 0.7858 - val_loss: 0.8022\n",
            "Epoch 306/500\n",
            " - 2s - loss: 0.7858 - val_loss: 0.8021\n",
            "Epoch 307/500\n",
            " - 2s - loss: 0.7855 - val_loss: 0.8022\n",
            "Epoch 308/500\n",
            " - 2s - loss: 0.7858 - val_loss: 0.8022\n",
            "Epoch 309/500\n",
            " - 2s - loss: 0.7857 - val_loss: 0.8022\n",
            "Epoch 310/500\n",
            " - 2s - loss: 0.7854 - val_loss: 0.8022\n",
            "Epoch 311/500\n",
            " - 2s - loss: 0.7857 - val_loss: 0.8023\n",
            "Epoch 312/500\n",
            " - 2s - loss: 0.7856 - val_loss: 0.8022\n",
            "Epoch 313/500\n",
            " - 2s - loss: 0.7854 - val_loss: 0.8022\n",
            "Epoch 314/500\n",
            " - 2s - loss: 0.7857 - val_loss: 0.8022\n",
            "Epoch 315/500\n",
            " - 2s - loss: 0.7856 - val_loss: 0.8021\n",
            "Epoch 316/500\n",
            " - 2s - loss: 0.7853 - val_loss: 0.8021\n",
            "Epoch 317/500\n",
            " - 2s - loss: 0.7856 - val_loss: 0.8022\n",
            "Epoch 318/500\n",
            " - 2s - loss: 0.7855 - val_loss: 0.8021\n",
            "Epoch 319/500\n",
            " - 2s - loss: 0.7853 - val_loss: 0.8022\n",
            "Epoch 320/500\n",
            " - 2s - loss: 0.7855 - val_loss: 0.8022\n",
            "Epoch 321/500\n",
            " - 2s - loss: 0.7855 - val_loss: 0.8022\n",
            "Epoch 322/500\n",
            " - 2s - loss: 0.7852 - val_loss: 0.8022\n",
            "Epoch 323/500\n",
            " - 2s - loss: 0.7855 - val_loss: 0.8022\n",
            "Epoch 324/500\n",
            " - 2s - loss: 0.7854 - val_loss: 0.8022\n",
            "Epoch 325/500\n",
            " - 2s - loss: 0.7851 - val_loss: 0.8022\n",
            "Epoch 326/500\n",
            " - 2s - loss: 0.7854 - val_loss: 0.8023\n",
            "Epoch 327/500\n",
            " - 2s - loss: 0.7853 - val_loss: 0.8023\n",
            "Epoch 328/500\n",
            " - 2s - loss: 0.7851 - val_loss: 0.8023\n",
            "Epoch 329/500\n",
            " - 2s - loss: 0.7854 - val_loss: 0.8023\n",
            "Epoch 330/500\n",
            " - 2s - loss: 0.7853 - val_loss: 0.8022\n",
            "Epoch 331/500\n",
            " - 2s - loss: 0.7851 - val_loss: 0.8023\n",
            "Epoch 332/500\n",
            " - 2s - loss: 0.7854 - val_loss: 0.8023\n",
            "Epoch 333/500\n",
            " - 2s - loss: 0.7853 - val_loss: 0.8023\n",
            "Epoch 334/500\n",
            " - 2s - loss: 0.7851 - val_loss: 0.8023\n",
            "Epoch 335/500\n",
            " - 2s - loss: 0.7853 - val_loss: 0.8024\n",
            "Epoch 336/500\n",
            " - 2s - loss: 0.7852 - val_loss: 0.8023\n",
            "Epoch 337/500\n",
            " - 2s - loss: 0.7850 - val_loss: 0.8023\n",
            "Epoch 338/500\n",
            " - 2s - loss: 0.7852 - val_loss: 0.8023\n",
            "Epoch 339/500\n",
            " - 2s - loss: 0.7851 - val_loss: 0.8023\n",
            "Epoch 340/500\n",
            " - 2s - loss: 0.7850 - val_loss: 0.8023\n",
            "Epoch 341/500\n",
            " - 2s - loss: 0.7852 - val_loss: 0.8023\n",
            "Epoch 342/500\n",
            " - 2s - loss: 0.7851 - val_loss: 0.8023\n",
            "Epoch 343/500\n",
            " - 2s - loss: 0.7849 - val_loss: 0.8023\n",
            "Epoch 344/500\n",
            " - 2s - loss: 0.7851 - val_loss: 0.8023\n",
            "Epoch 345/500\n",
            " - 2s - loss: 0.7850 - val_loss: 0.8023\n",
            "Epoch 346/500\n",
            " - 2s - loss: 0.7849 - val_loss: 0.8023\n",
            "Epoch 347/500\n",
            " - 2s - loss: 0.7851 - val_loss: 0.8023\n",
            "Epoch 348/500\n",
            " - 2s - loss: 0.7850 - val_loss: 0.8023\n",
            "Epoch 349/500\n",
            " - 2s - loss: 0.7849 - val_loss: 0.8024\n",
            "Epoch 350/500\n",
            " - 2s - loss: 0.7850 - val_loss: 0.8023\n",
            "Epoch 351/500\n",
            " - 2s - loss: 0.7848 - val_loss: 0.8024\n",
            "Epoch 352/500\n",
            " - 2s - loss: 0.7850 - val_loss: 0.8023\n",
            "Epoch 353/500\n",
            " - 2s - loss: 0.7850 - val_loss: 0.8023\n",
            "Epoch 354/500\n",
            " - 2s - loss: 0.7849 - val_loss: 0.8023\n",
            "Epoch 355/500\n",
            " - 2s - loss: 0.7850 - val_loss: 0.8024\n",
            "Epoch 356/500\n",
            " - 2s - loss: 0.7850 - val_loss: 0.8023\n",
            "Epoch 357/500\n",
            " - 2s - loss: 0.7848 - val_loss: 0.8023\n",
            "Epoch 358/500\n",
            " - 2s - loss: 0.7850 - val_loss: 0.8024\n",
            "Epoch 359/500\n",
            " - 2s - loss: 0.7849 - val_loss: 0.8024\n",
            "Epoch 360/500\n",
            " - 2s - loss: 0.7847 - val_loss: 0.8024\n",
            "Epoch 361/500\n",
            " - 2s - loss: 0.7849 - val_loss: 0.8024\n",
            "Epoch 362/500\n",
            " - 2s - loss: 0.7848 - val_loss: 0.8024\n",
            "Epoch 363/500\n",
            " - 2s - loss: 0.7847 - val_loss: 0.8024\n",
            "Epoch 364/500\n",
            " - 2s - loss: 0.7849 - val_loss: 0.8024\n",
            "Epoch 365/500\n",
            " - 2s - loss: 0.7848 - val_loss: 0.8024\n",
            "Epoch 366/500\n",
            " - 2s - loss: 0.7847 - val_loss: 0.8024\n",
            "Epoch 367/500\n",
            " - 2s - loss: 0.7848 - val_loss: 0.8025\n",
            "Epoch 368/500\n",
            " - 2s - loss: 0.7847 - val_loss: 0.8024\n",
            "Epoch 369/500\n",
            " - 2s - loss: 0.7846 - val_loss: 0.8024\n",
            "Epoch 370/500\n",
            " - 2s - loss: 0.7848 - val_loss: 0.8025\n",
            "Epoch 371/500\n",
            " - 2s - loss: 0.7847 - val_loss: 0.8025\n",
            "Epoch 372/500\n",
            " - 2s - loss: 0.7846 - val_loss: 0.8024\n",
            "Epoch 373/500\n",
            " - 2s - loss: 0.7848 - val_loss: 0.8024\n",
            "Epoch 374/500\n",
            " - 2s - loss: 0.7846 - val_loss: 0.8024\n",
            "Epoch 375/500\n",
            " - 2s - loss: 0.7846 - val_loss: 0.8025\n",
            "Epoch 376/500\n",
            " - 2s - loss: 0.7847 - val_loss: 0.8025\n",
            "Epoch 377/500\n",
            " - 2s - loss: 0.7846 - val_loss: 0.8025\n",
            "Epoch 378/500\n",
            " - 2s - loss: 0.7846 - val_loss: 0.8024\n",
            "Epoch 379/500\n",
            " - 2s - loss: 0.7847 - val_loss: 0.8025\n",
            "Epoch 380/500\n",
            " - 2s - loss: 0.7846 - val_loss: 0.8025\n",
            "Epoch 381/500\n",
            " - 2s - loss: 0.7845 - val_loss: 0.8024\n",
            "Epoch 382/500\n",
            " - 2s - loss: 0.7847 - val_loss: 0.8025\n",
            "Epoch 383/500\n",
            " - 2s - loss: 0.7846 - val_loss: 0.8024\n",
            "Epoch 384/500\n",
            " - 2s - loss: 0.7845 - val_loss: 0.8024\n",
            "Epoch 385/500\n",
            " - 2s - loss: 0.7846 - val_loss: 0.8025\n",
            "Epoch 386/500\n",
            " - 2s - loss: 0.7845 - val_loss: 0.8025\n",
            "Epoch 387/500\n",
            " - 2s - loss: 0.7844 - val_loss: 0.8024\n",
            "Epoch 388/500\n",
            " - 2s - loss: 0.7846 - val_loss: 0.8025\n",
            "Epoch 389/500\n",
            " - 2s - loss: 0.7845 - val_loss: 0.8024\n",
            "Epoch 390/500\n",
            " - 2s - loss: 0.7844 - val_loss: 0.8023\n",
            "Epoch 391/500\n",
            " - 2s - loss: 0.7846 - val_loss: 0.8025\n",
            "Epoch 392/500\n",
            " - 2s - loss: 0.7844 - val_loss: 0.8024\n",
            "Epoch 393/500\n",
            " - 2s - loss: 0.7843 - val_loss: 0.8023\n",
            "Epoch 394/500\n",
            " - 2s - loss: 0.7845 - val_loss: 0.8024\n",
            "Epoch 395/500\n",
            " - 2s - loss: 0.7844 - val_loss: 0.8024\n",
            "Epoch 396/500\n",
            " - 2s - loss: 0.7843 - val_loss: 0.8024\n",
            "Epoch 397/500\n",
            " - 2s - loss: 0.7845 - val_loss: 0.8024\n",
            "Epoch 398/500\n",
            " - 2s - loss: 0.7843 - val_loss: 0.8024\n",
            "Epoch 399/500\n",
            " - 2s - loss: 0.7844 - val_loss: 0.8024\n",
            "Epoch 400/500\n",
            " - 2s - loss: 0.7844 - val_loss: 0.8025\n",
            "Epoch 401/500\n",
            " - 2s - loss: 0.7845 - val_loss: 0.8025\n",
            "Epoch 402/500\n",
            " - 2s - loss: 0.7846 - val_loss: 0.8022\n",
            "Epoch 403/500\n",
            " - 2s - loss: 0.7845 - val_loss: 0.8025\n",
            "Epoch 404/500\n",
            " - 2s - loss: 0.7845 - val_loss: 0.8023\n",
            "Epoch 405/500\n",
            " - 2s - loss: 0.7842 - val_loss: 0.8024\n",
            "Epoch 406/500\n",
            " - 2s - loss: 0.7843 - val_loss: 0.8024\n",
            "Epoch 407/500\n",
            " - 2s - loss: 0.7843 - val_loss: 0.8024\n",
            "Epoch 408/500\n",
            " - 2s - loss: 0.7841 - val_loss: 0.8024\n",
            "Epoch 409/500\n",
            " - 2s - loss: 0.7844 - val_loss: 0.8024\n",
            "Epoch 410/500\n",
            " - 2s - loss: 0.7843 - val_loss: 0.8023\n",
            "Epoch 411/500\n",
            " - 2s - loss: 0.7842 - val_loss: 0.8023\n",
            "Epoch 412/500\n",
            " - 2s - loss: 0.7843 - val_loss: 0.8024\n",
            "Epoch 413/500\n",
            " - 2s - loss: 0.7842 - val_loss: 0.8024\n",
            "Epoch 414/500\n",
            " - 2s - loss: 0.7841 - val_loss: 0.8024\n",
            "Epoch 415/500\n",
            " - 2s - loss: 0.7843 - val_loss: 0.8025\n",
            "Epoch 416/500\n",
            " - 2s - loss: 0.7843 - val_loss: 0.8024\n",
            "Epoch 417/500\n",
            " - 2s - loss: 0.7841 - val_loss: 0.8024\n",
            "Epoch 418/500\n",
            " - 2s - loss: 0.7842 - val_loss: 0.8024\n",
            "Epoch 419/500\n",
            " - 2s - loss: 0.7840 - val_loss: 0.8024\n",
            "Epoch 420/500\n",
            " - 2s - loss: 0.7842 - val_loss: 0.8023\n",
            "Epoch 421/500\n",
            " - 2s - loss: 0.7842 - val_loss: 0.8024\n",
            "Epoch 422/500\n",
            " - 2s - loss: 0.7840 - val_loss: 0.8023\n",
            "Epoch 423/500\n",
            " - 2s - loss: 0.7842 - val_loss: 0.8024\n",
            "Epoch 424/500\n",
            " - 2s - loss: 0.7842 - val_loss: 0.8023\n",
            "Epoch 425/500\n",
            " - 2s - loss: 0.7839 - val_loss: 0.8023\n",
            "Epoch 426/500\n",
            " - 2s - loss: 0.7842 - val_loss: 0.8024\n",
            "Epoch 427/500\n",
            " - 2s - loss: 0.7841 - val_loss: 0.8024\n",
            "Epoch 428/500\n",
            " - 2s - loss: 0.7840 - val_loss: 0.8024\n",
            "Epoch 429/500\n",
            " - 2s - loss: 0.7841 - val_loss: 0.8024\n",
            "Epoch 430/500\n",
            " - 2s - loss: 0.7841 - val_loss: 0.8024\n",
            "Epoch 431/500\n",
            " - 2s - loss: 0.7839 - val_loss: 0.8025\n",
            "Epoch 432/500\n",
            " - 2s - loss: 0.7841 - val_loss: 0.8025\n",
            "Epoch 433/500\n",
            " - 2s - loss: 0.7841 - val_loss: 0.8024\n",
            "Epoch 434/500\n",
            " - 2s - loss: 0.7839 - val_loss: 0.8025\n",
            "Epoch 435/500\n",
            " - 2s - loss: 0.7841 - val_loss: 0.8025\n",
            "Epoch 436/500\n",
            " - 2s - loss: 0.7840 - val_loss: 0.8025\n",
            "Epoch 437/500\n",
            " - 2s - loss: 0.7839 - val_loss: 0.8025\n",
            "Epoch 438/500\n",
            " - 2s - loss: 0.7841 - val_loss: 0.8025\n",
            "Epoch 439/500\n",
            " - 2s - loss: 0.7840 - val_loss: 0.8023\n",
            "Epoch 440/500\n",
            " - 2s - loss: 0.7838 - val_loss: 0.8024\n",
            "Epoch 441/500\n",
            " - 2s - loss: 0.7840 - val_loss: 0.8025\n",
            "Epoch 442/500\n",
            " - 2s - loss: 0.7840 - val_loss: 0.8024\n",
            "Epoch 443/500\n",
            " - 2s - loss: 0.7838 - val_loss: 0.8024\n",
            "Epoch 444/500\n",
            " - 2s - loss: 0.7840 - val_loss: 0.8025\n",
            "Epoch 445/500\n",
            " - 2s - loss: 0.7840 - val_loss: 0.8024\n",
            "Epoch 446/500\n",
            " - 2s - loss: 0.7838 - val_loss: 0.8026\n",
            "Epoch 447/500\n",
            " - 2s - loss: 0.7838 - val_loss: 0.8025\n",
            "Epoch 448/500\n",
            " - 2s - loss: 0.7840 - val_loss: 0.8025\n",
            "Epoch 449/500\n",
            " - 2s - loss: 0.7839 - val_loss: 0.8024\n",
            "Epoch 450/500\n",
            " - 2s - loss: 0.7837 - val_loss: 0.8024\n",
            "Epoch 451/500\n",
            " - 2s - loss: 0.7839 - val_loss: 0.8025\n",
            "Epoch 452/500\n",
            " - 2s - loss: 0.7839 - val_loss: 0.8024\n",
            "Epoch 453/500\n",
            " - 2s - loss: 0.7837 - val_loss: 0.8024\n",
            "Epoch 454/500\n",
            " - 2s - loss: 0.7839 - val_loss: 0.8024\n",
            "Epoch 455/500\n",
            " - 2s - loss: 0.7839 - val_loss: 0.8024\n",
            "Epoch 456/500\n",
            " - 2s - loss: 0.7837 - val_loss: 0.8025\n",
            "Epoch 457/500\n",
            " - 2s - loss: 0.7837 - val_loss: 0.8024\n",
            "Epoch 458/500\n",
            " - 2s - loss: 0.7839 - val_loss: 0.8025\n",
            "Epoch 459/500\n",
            " - 2s - loss: 0.7839 - val_loss: 0.8023\n",
            "Epoch 460/500\n",
            " - 2s - loss: 0.7837 - val_loss: 0.8025\n",
            "Epoch 461/500\n",
            " - 2s - loss: 0.7837 - val_loss: 0.8024\n",
            "Epoch 462/500\n",
            " - 2s - loss: 0.7839 - val_loss: 0.8024\n",
            "Epoch 463/500\n",
            " - 2s - loss: 0.7838 - val_loss: 0.8024\n",
            "Epoch 464/500\n",
            " - 2s - loss: 0.7836 - val_loss: 0.8024\n",
            "Epoch 465/500\n",
            " - 2s - loss: 0.7839 - val_loss: 0.8025\n",
            "Epoch 466/500\n",
            " - 2s - loss: 0.7838 - val_loss: 0.8023\n",
            "Epoch 467/500\n",
            " - 2s - loss: 0.7836 - val_loss: 0.8024\n",
            "Epoch 468/500\n",
            " - 2s - loss: 0.7837 - val_loss: 0.8024\n",
            "Epoch 469/500\n",
            " - 2s - loss: 0.7838 - val_loss: 0.8024\n",
            "Epoch 470/500\n",
            " - 2s - loss: 0.7837 - val_loss: 0.8024\n",
            "Epoch 471/500\n",
            " - 2s - loss: 0.7835 - val_loss: 0.8024\n",
            "Epoch 472/500\n",
            " - 2s - loss: 0.7838 - val_loss: 0.8024\n",
            "Epoch 473/500\n",
            " - 2s - loss: 0.7837 - val_loss: 0.8024\n",
            "Epoch 474/500\n",
            " - 2s - loss: 0.7835 - val_loss: 0.8025\n",
            "Epoch 475/500\n",
            " - 2s - loss: 0.7836 - val_loss: 0.8024\n",
            "Epoch 476/500\n",
            " - 2s - loss: 0.7838 - val_loss: 0.8024\n",
            "Epoch 477/500\n",
            " - 2s - loss: 0.7837 - val_loss: 0.8023\n",
            "Epoch 478/500\n",
            " - 2s - loss: 0.7835 - val_loss: 0.8024\n",
            "Epoch 479/500\n",
            " - 2s - loss: 0.7837 - val_loss: 0.8024\n",
            "Epoch 480/500\n",
            " - 2s - loss: 0.7836 - val_loss: 0.8023\n",
            "Epoch 481/500\n",
            " - 2s - loss: 0.7834 - val_loss: 0.8023\n",
            "Epoch 482/500\n",
            " - 2s - loss: 0.7836 - val_loss: 0.8024\n",
            "Epoch 483/500\n",
            " - 2s - loss: 0.7835 - val_loss: 0.8025\n",
            "Epoch 484/500\n",
            " - 2s - loss: 0.7834 - val_loss: 0.8025\n",
            "Epoch 485/500\n",
            " - 2s - loss: 0.7836 - val_loss: 0.8024\n",
            "Epoch 486/500\n",
            " - 2s - loss: 0.7834 - val_loss: 0.8024\n",
            "Epoch 487/500\n",
            " - 2s - loss: 0.7836 - val_loss: 0.8024\n",
            "Epoch 488/500\n",
            " - 2s - loss: 0.7834 - val_loss: 0.8023\n",
            "Epoch 489/500\n",
            " - 2s - loss: 0.7836 - val_loss: 0.8024\n",
            "Epoch 490/500\n",
            " - 2s - loss: 0.7835 - val_loss: 0.8024\n",
            "Epoch 491/500\n",
            " - 2s - loss: 0.7833 - val_loss: 0.8024\n",
            "Epoch 492/500\n",
            " - 2s - loss: 0.7835 - val_loss: 0.8023\n",
            "Epoch 493/500\n",
            " - 2s - loss: 0.7833 - val_loss: 0.8024\n",
            "Epoch 494/500\n",
            " - 2s - loss: 0.7835 - val_loss: 0.8023\n",
            "Epoch 495/500\n",
            " - 2s - loss: 0.7833 - val_loss: 0.8024\n",
            "Epoch 496/500\n",
            " - 2s - loss: 0.7835 - val_loss: 0.8023\n",
            "Epoch 497/500\n",
            " - 2s - loss: 0.7834 - val_loss: 0.8024\n",
            "Epoch 498/500\n",
            " - 2s - loss: 0.7834 - val_loss: 0.8024\n",
            "Epoch 499/500\n",
            " - 2s - loss: 0.7833 - val_loss: 0.8023\n",
            "Epoch 500/500\n",
            " - 2s - loss: 0.7836 - val_loss: 0.8023\n",
            "CPU times: user 22min 25s, sys: 2min 10s, total: 24min 36s\n",
            "Wall time: 15min 40s\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x7f7e32a97e80>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 65
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6lb6KgsITNOB",
        "colab_type": "code",
        "outputId": "95a1c390-918b-4cd8-93d6-58d1ca095660",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "#CRPS of train and test data 500 epochs ensemble mean and sd\n",
        "(hidden_model.evaluate(train_std_df_X, train_y, batch_size = 50, verbose=0), hidden_model.evaluate(test_std_df_X, test_y, batch_size = 50, verbose=0))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(0.7890088468215171, 0.8033952380106306)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 66
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0rzdExPD2w4k",
        "colab_type": "text"
      },
      "source": [
        "## Feature importance for NN model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oUZS8RiMt6x9",
        "colab_type": "code",
        "outputId": "07e6f509-accd-4daa-e1c3-40e940fd9858",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 404
        }
      },
      "source": [
        "#fimp for nn with distance to coast 500 eposchs\n",
        "\n",
        "ref_score = hidden_model.evaluate(test_std_df_X, test_y, batch_size = 50, verbose=0)\n",
        "fimp_nn_standardized_model = perm_imp(hidden_model)\n",
        "fig, ax = plt.subplots(figsize=(12, 5))\n",
        "sns.barplot(data=fimp_nn_standardized_model, y='Importance', x='Feature', ax=ax)\n",
        "plt.xticks(rotation=90);"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAscAAAGDCAYAAADH173JAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\ns8zBsZmZmZlZ5uDYzMzMzCz7L6gbSfaddEhJAAAAAElFTkSuQmCC\n",
            "text/plain": [
              "<Figure size 864x360 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ug5aG6IOKdF-",
        "colab_type": "text"
      },
      "source": [
        "## Feature importance tabular form for FC/LR and NN models"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "W422eEvJ1Og7",
        "colab_type": "code",
        "outputId": "abe34ac8-dff1-4c0f-cb36-373020f8e347",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 865
        }
      },
      "source": [
        "# Feature importance with distance to sea another run\n",
        "comb_nn_df = pd.DataFrame(data=fimp_nn_standardized_model['Feature']); \n",
        "comb_nn_df['nn_std_importance'] = fimp_nn_standardized_model['Importance']\n",
        "comb_nn_df.sort_values('nn_std_importance', ascending=False, inplace=True)\n",
        "comb_nn_df['FC/LR_std_importance'] = fimp_fc_standardized_model['Importance']\n",
        "comb_nn_df.head(30)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Feature</th>\n",
              "      <th>nn_std_importance</th>\n",
              "      <th>FC/LR_std_importance</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>T2mensmean</td>\n",
              "      <td>7.294589</td>\n",
              "      <td>8.528208</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9</th>\n",
              "      <td>T.L850ensmean</td>\n",
              "      <td>0.447305</td>\n",
              "      <td>0.126460</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>T0mensmean</td>\n",
              "      <td>0.354539</td>\n",
              "      <td>0.209848</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>11</th>\n",
              "      <td>T.L925ensmean</td>\n",
              "      <td>0.347389</td>\n",
              "      <td>0.145047</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>dELEV</td>\n",
              "      <td>0.202566</td>\n",
              "      <td>0.134191</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>H2mensmean</td>\n",
              "      <td>0.098765</td>\n",
              "      <td>0.034140</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>13</th>\n",
              "      <td>WS10ensmean</td>\n",
              "      <td>0.070620</td>\n",
              "      <td>0.002470</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>T2menssd</td>\n",
              "      <td>0.054949</td>\n",
              "      <td>0.034290</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>25</th>\n",
              "      <td>RR_6</td>\n",
              "      <td>0.054498</td>\n",
              "      <td>0.007640</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>T0menssd</td>\n",
              "      <td>0.043573</td>\n",
              "      <td>0.000974</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>y</td>\n",
              "      <td>0.031526</td>\n",
              "      <td>0.002362</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>21</th>\n",
              "      <td>LCCensmean</td>\n",
              "      <td>0.026331</td>\n",
              "      <td>0.001107</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>23</th>\n",
              "      <td>MSLPensmean</td>\n",
              "      <td>0.023201</td>\n",
              "      <td>0.001648</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8</th>\n",
              "      <td>H2menssd</td>\n",
              "      <td>0.017265</td>\n",
              "      <td>0.001114</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>x</td>\n",
              "      <td>0.014231</td>\n",
              "      <td>0.002062</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>17</th>\n",
              "      <td>TCCensmean</td>\n",
              "      <td>0.014230</td>\n",
              "      <td>0.003259</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>19</th>\n",
              "      <td>MCCensmean</td>\n",
              "      <td>0.011270</td>\n",
              "      <td>0.003537</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>15</th>\n",
              "      <td>WD10ensmean</td>\n",
              "      <td>0.007879</td>\n",
              "      <td>0.000378</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>24</th>\n",
              "      <td>MSLPenssd</td>\n",
              "      <td>0.005397</td>\n",
              "      <td>0.000071</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>12</th>\n",
              "      <td>T.L925enssd</td>\n",
              "      <td>0.005351</td>\n",
              "      <td>0.001229</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>22</th>\n",
              "      <td>LCCenssd</td>\n",
              "      <td>0.004951</td>\n",
              "      <td>0.000880</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>20</th>\n",
              "      <td>MCCenssd</td>\n",
              "      <td>0.004528</td>\n",
              "      <td>0.000077</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>16</th>\n",
              "      <td>WD10enssd</td>\n",
              "      <td>0.003275</td>\n",
              "      <td>0.000751</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>18</th>\n",
              "      <td>TCCenssd</td>\n",
              "      <td>0.003012</td>\n",
              "      <td>-0.000020</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>14</th>\n",
              "      <td>WS10enssd</td>\n",
              "      <td>0.002758</td>\n",
              "      <td>0.002053</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>10</th>\n",
              "      <td>T.L850enssd</td>\n",
              "      <td>0.002597</td>\n",
              "      <td>0.000105</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "          Feature  nn_std_importance  FC/LR_std_importance\n",
              "5      T2mensmean           7.294589              8.528208\n",
              "9   T.L850ensmean           0.447305              0.126460\n",
              "3      T0mensmean           0.354539              0.209848\n",
              "11  T.L925ensmean           0.347389              0.145047\n",
              "2           dELEV           0.202566              0.134191\n",
              "7      H2mensmean           0.098765              0.034140\n",
              "13    WS10ensmean           0.070620              0.002470\n",
              "6        T2menssd           0.054949              0.034290\n",
              "25           RR_6           0.054498              0.007640\n",
              "4        T0menssd           0.043573              0.000974\n",
              "1               y           0.031526              0.002362\n",
              "21     LCCensmean           0.026331              0.001107\n",
              "23    MSLPensmean           0.023201              0.001648\n",
              "8        H2menssd           0.017265              0.001114\n",
              "0               x           0.014231              0.002062\n",
              "17     TCCensmean           0.014230              0.003259\n",
              "19     MCCensmean           0.011270              0.003537\n",
              "15    WD10ensmean           0.007879              0.000378\n",
              "24      MSLPenssd           0.005397              0.000071\n",
              "12    T.L925enssd           0.005351              0.001229\n",
              "22       LCCenssd           0.004951              0.000880\n",
              "20       MCCenssd           0.004528              0.000077\n",
              "16      WD10enssd           0.003275              0.000751\n",
              "18       TCCenssd           0.003012             -0.000020\n",
              "14      WS10enssd           0.002758              0.002053\n",
              "10    T.L850enssd           0.002597              0.000105"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 68
        }
      ]
    }
  ]
}